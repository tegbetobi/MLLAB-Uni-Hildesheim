{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "## Preprocessing Airfare dataset\n",
    "The data is loaded in tictactoe dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "colnames=['top-left-square','top-middle-square','top-right-square','middle-left-square','middle-middle-square','middle-right-square','bottom-left-square','bottom-middle-square','bottom-right-square','Class']\n",
    "filename = r\"E:\\Documents\\University of Hildesheim\\Machine learning lab\\lab4\\tic-tac-toe.data\"\n",
    "tictactoe = pd.read_csv(filename,delimiter=',',header=None,names=colnames)\n",
    "tictactoe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "### The dataset has no missing values and all the columns with object values, these could be treated as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe = tictactoe.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be experimented with is a simple categorical encoding, wherein each unique entry is assigned it's own number. Pandas does with relative ease by assigning desired object columns to a category dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe=tictactoe.apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is shown that each categorical column has its unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By counting the number of positive and negative we can see that the datasets has more number of postive than negative values hence it is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The solution to make the dataset balanced is using stratified sampling.Stratified sampling is a sampling method where the researcher divides the population into separate groups which is called strata then a probability sample is drawn from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_sample(df, col, samples):\n",
    "    n = min(samples, df[col].value_counts().min())\n",
    "    df_ = df.groupby(col).apply(lambda x: x.sample(n))\n",
    "    df_.index = df_.index.droplevel(0)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe_stratified=stratify_sample(tictactoe, 'Class', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After stratified sampling the number of positive and negative is equal which makes the dataset balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe_stratified['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe_stratified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['top-left-square','top-middle-square','top-right-square','middle-left-square','middle-middle-square','middle-right-square','bottom-left-square','bottom-middle-square','bottom-right-square']\n",
    "Xdata = tictactoe_stratified[features]\n",
    "Ydata = tictactoe_stratified['Class']\n",
    "\n",
    "x_train, x_test, y_train, y_test =train_test_split(Xdata,Ydata,train_size=0.8,test_size=0.2,random_state=0)\n",
    "\n",
    "print('x_train :',x_train.shape)\n",
    "print('x_test :',x_test.shape)\n",
    "print('y_train :',y_train.shape)\n",
    "print('y_test :',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression is a method in machine learning for classification problems to output discrete values\n",
    "### Step length bolddriver algorithm function\n",
    "The Bold Driver Heuristic makes the assumption that smaller step sizes are needed when closer to the optimum.It adjusts the step size based on the value of f (x) at time t.If the value of f (x) grows, the step size must decrease.If the value of f (x) decreases, the step size can be larger for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bolddriver(x_train,y_train,beta_hat,lr,mhuplus = 1.1, mhuminus = 0.5):\n",
    "    \n",
    "    lr = lr*mhuplus\n",
    "    contiter = True\n",
    "    iterations = 0\n",
    "    \n",
    "    y_hat=logistic_function(x_train,beta_hat) # The current predicted value of Y\n",
    "    l_old=log_likelihood(x_train, y_train, beta_hat) \n",
    "    \n",
    "    while contiter:\n",
    "\n",
    "        grad_beta = (np.dot(np.transpose(x_train), y_train - y_hat))\n",
    "        beta_hat = beta_hat+learn_rate*(np.dot(np.transpose(x_train), y_train - y_hat))\n",
    "        \n",
    "\n",
    "        y_hat=logistic_function(x_train,beta_hat) # The current predicted value of Y\n",
    "        l=log_likelihood(x_train, y_train, beta_hat)\n",
    "        \n",
    "        \n",
    "        if l - l_old <= 0:\n",
    "            lr = lr*mhuminus\n",
    "            iterations += 1\n",
    "        else:\n",
    "            return lr\n",
    "        \n",
    "        if iterations == 250:\n",
    "            break\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(x, y, beta):\n",
    "    z = np.dot(x, beta)\n",
    "    log = np.sum( y*z - np.log(1 + np.exp(z)) )\n",
    "    return log\n",
    "\n",
    "def gradient_ascent(X, h, y):\n",
    "    return np.dot(X.T, y - h)\n",
    "\n",
    "def logistic_function(X, beta):\n",
    "    z = x_train.dot(beta)\n",
    "    return 1 / (1 + np.exp(-z))   \n",
    "\n",
    "logloss = lambda y,ypred: np.mean((y*np.log(ypred)+(1-y)*np.log(1-ypred)))\n",
    "cost = lambda y,ypred: np.mean((y - ypred)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression using stochastic gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train,n_features = x_train.shape\n",
    "\n",
    "num_iter     = 1000\n",
    "learn_rate   = 0.00001\n",
    "\n",
    "beta_hat     = np.random.random(n_features).reshape(-1,1)\n",
    "relative_l= []\n",
    "relative_loss=[]\n",
    "loglosstest=[]\n",
    "\n",
    "y_hat=logistic_function(x_train,beta_hat)\n",
    "l=0\n",
    "l_old=log_likelihood(x_train, y_train, beta_hat)\n",
    "\n",
    "chunk_size = 20\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "        loss_old  = cost(y_train,y_hat)\n",
    "    \n",
    "        for chunk in range(len(x_train)//chunk_size):\n",
    "            \n",
    "            x_chunk = x_train[chunk*chunk_size:min((chunk+1)*chunk_size,len(x_train))]\n",
    "            y_chunk = y_train[chunk*chunk_size:min((chunk+1)*chunk_size,len(y_train))]\n",
    "            \n",
    "            y_hat=logistic_function(x_chunk,beta_hat)\n",
    "            \n",
    "            beta_hat = beta_hat+learn_rate*(np.dot(np.transpose(x_chunk), y_chunk - y_hat))\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        learn_rate =  bolddriver(x_train,y_train,beta_hat,learn_rate,mhuplus = 1.1, mhuminus = 0.5)\n",
    "    \n",
    "        y_hat=logistic_function(x_train,beta_hat)\n",
    "    \n",
    "        loss_new  = cost(y_train,y_hat)\n",
    "    \n",
    "    \n",
    "        l_old=l\n",
    "        l=log_likelihood(x_train, y_train, beta_hat)\n",
    "\n",
    "        relative_l.append(l-l_old)\n",
    "        relative_loss.append(np.abs(loss_new.values-loss_old.values))\n",
    "        loglosstest.append(logloss(y_test,y_hat))\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\"epochs: {i} log_likelihood: {(l-l_old)} learning rate: {learn_rate} loss:{np.abs(loss_new.values-loss_old.values)} Logloss : {logloss(y_test,y_hat).values}\")\n",
    "\n",
    "        last_epoch=i+1\n",
    "\n",
    "        if np.abs(l-l_old) == 0:\n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"log_likelihood\")\n",
    "plt.plot( np.arange(last_epoch),relative_l,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"differential cost\")\n",
    "plt.plot( np.arange(last_epoch),relative_loss,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"logloss\")\n",
    "plt.plot( np.arange(last_epoch),loglosstest,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Newton Algorithm (learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method is a second-order optimization algorithm that can help us find the best weights in our logistic function in fewer iterations compared to batch gradient descent.\n",
    "\n",
    "The generalization of Newtonâ€™s method to a multidimensional setting (also called the Newton-Raphson method) is given by:\n",
    "\n",
    "Where the Hessian is represented by:\n",
    "\n",
    "For Logistic Regression, the Hessian is given by:\n",
    "\n",
    "$$\n",
    "Hf(\\beta) = -X^TWX\n",
    "$$\n",
    "and the gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\beta) = X^T(y-p)\n",
    "$$\n",
    "where$$\n",
    "W := \\text{diag}\\left(p(1-p)\\right)\n",
    "$$\n",
    "and $p$ are the predicted probabilites computed at the current value of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(beta0, y, X, lr):\n",
    "    \n",
    "    p = np.array(sigmoid(X.dot(beta0[:,0]))).T  \n",
    "    W = np.diag((p*(1-p))) \n",
    "    \n",
    "    \n",
    "    hess = np.dot((np.dot(X.T,W)),X)  \n",
    "    grad = (np.transpose(X)).dot(y-p)  \n",
    "     \n",
    "    \n",
    "    s =lr*(np.dot(np.linalg.inv(hess), grad))      \n",
    "    beta = beta0 + s\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Newton method we noticed that the convergence is faster because it needed only 5 iteration to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 100\n",
    " \n",
    "lr =0.001\n",
    "\n",
    "relative_l_newton=[]\n",
    "relative_loss_newton=[]\n",
    "loglosstest_newton=[]\n",
    "\n",
    "beta_old, beta = np.ones((n_features,1)), np.zeros((n_features,1))\n",
    "l_newton=0\n",
    "y_hat_newton=logistic_function(x_train,beta_old)\n",
    "l_old_newton=-log_likelihood(x_train, y_train, beta_old)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    loss_old_newton  = cost(y_train,y_hat_newton)\n",
    "    beta_old = beta\n",
    "    \n",
    "    beta = newton_step(beta, y_train, x_train, reg_term)\n",
    "    y_hat_newton=logistic_function(x_train,beta)\n",
    "    \n",
    "    loss_new_newton  = cost(y_train,y_hat_newton)\n",
    "    \n",
    "    l_old_newton=l_newton\n",
    "    l_newton=-log_likelihood(x_train, y_train, beta)\n",
    "    relative_l_newton.append(l_newton-l_old_newton)\n",
    "    relative_loss_newton.append(np.abs(loss_new_newton.values-loss_old_newton.values))\n",
    "    \n",
    "   \n",
    "    loglosstest_newton.append(-logloss(y_test,y_hat_newton))\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "        print(f\"epochs: {i} log_likelihood: {(l_newton-l_old_newton)} loss:{np.abs(loss_new_newton.values-loss_old_newton.values)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if np.abs(l_newton-l_old_newton) == 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"log_likelihood\")\n",
    "plt.plot( np.arange(len(relative_l_newton)),relative_l_newton,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.plot( np.arange(len(relative_loss_newton)),relative_loss_newton,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"logloss\")\n",
    "plt.plot( np.arange(len(loglosstest_newton)),loglosstest_newton,'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
