{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02872b8-4ab8-416b-96fa-f63393a95697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f4b94-10f7-4742-864e-9c20708abe5f",
   "metadata": {},
   "source": [
    "### 1. SVM with Submanifold Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3fc6e0-d05c-431f-b29f-4f50535ca59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Map labels to +1 and -1\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Submanifold Minimization Algorithm\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    # Update weights for correctly classified points\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    # Update weights and bias for misclassified points\n",
    "                    self.w -= self.lr * (\n",
    "                        2 * self.lambda_param * self.w - np.dot(x_i, y_[idx])\n",
    "                    )\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adf5a65-39e5-4ef4-957e-7a2d2b6f49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.35074838 0.51297922]\n",
      "Bias: 1.313999999999966\n",
      "Predictions: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_pos = np.array([[2.0, 2.2], [2.7, 2.5], [2.3, 2.0], [3.1, 2.3], [2.5, 2.4], [2.8, 2.7]])\n",
    "y_pos = np.ones(len(X_pos))\n",
    "\n",
    "X_neg = np.array([[1.6, 1.5], [2.0, 1.9], [2.1, 1.8], [1.7, 1.6], [1.8, 1.7], [2.0, 1.6]])\n",
    "y_neg = -np.ones(len(X_neg))\n",
    "\n",
    "# Combining the  positive and negative classes\n",
    "X = np.vstack((X_pos, X_neg))\n",
    "y = np.hstack((y_pos, y_neg))\n",
    "\n",
    "clf = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Model parameters\n",
    "print(\"Weights:\", clf.w)\n",
    "print(\"Bias:\", clf.b)\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(X)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da8473-2af9-4f8b-be5c-307859ed3edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550f97dd-df7c-4ae2-8954-8dfdc41c8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "F1 Score: 0.6666666666666666\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y, predictions)\n",
    "f1 = f1_score(y, predictions)\n",
    "recall = recall_score(y, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec86d4c-4cf0-409b-bc48-4fedff202bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94d299-00d4-447b-a290-fec9f89fc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### code basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99025328-461c-4d76-8b0e-e3cebee5bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"rbf\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"linear\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Step 1: Import the dataset and display the first few rows\n",
    "df = pd.read_csv(\"breast_cancer_data.csv\")\n",
    "\n",
    "print(\"Number of rows and columns: \", df.shape)\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "df.head()\n",
    "\n",
    "df = pd.get_dummies(df, columns=['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'breast', 'breast-quad', 'irradiat'], drop_first=True)\n",
    "print(\"First few rows of the encoded dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29969802-a7ab-4e03-9478-4acd83f2aa91",
   "metadata": {},
   "source": [
    "### Task 2: Logistic Regression Model Without Handling Class Imbalance\n",
    "\n",
    "1. Split the dataset into training and test sets.\n",
    "2. Train a Logistic Regression model using the training data.\n",
    "3. Evaluate the model using a classification report, including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebd11c-400d-4221-afd9-58362305e8be",
   "metadata": {},
   "source": [
    "# Step 1: Split the dataset into training and test sets\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Step 2: Train a Logistic Regression model using the training data\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model using a classification report, including precision, recall, and F1-score\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report Without Handling Class Imbalance:\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddb664-ac21-4e19-b734-15214fd08967",
   "metadata": {},
   "source": [
    "### Task 3: Handling Class Imbalance Using Undersampling\n",
    "\n",
    "1. Apply undersampling to balance the classes in the training data.\n",
    "2. Train a Logistic Regression model using the undersampled training data.\n",
    "3. Evaluate the model using a classification report, including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46edcf3c-8783-4d2a-8c54-e2a8c3cfba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply undersampling to balance the classes in the training data\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 2: Train a Logistic Regression model using the undersampled training data\n",
    "model_rus = LogisticRegression(max_iter=2000)\n",
    "model_rus.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# Step 3: Evaluate the model using a classification report, including precision, recall, and F1-score\n",
    "y_pred_rus = model_rus.predict(X_test)\n",
    "report_rus = classification_report(y_test, y_pred_rus)\n",
    "print(\"Classification Report with Undersampling:\")\n",
    "print(report_rus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36fc833-5d80-441a-bc09-6ea9623dcce1",
   "metadata": {},
   "source": [
    "### Task 4: Handling Class Imbalance Using SMOTE (Oversampling)\n",
    "\n",
    "1. Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance the classes in the training data.\n",
    "2. Train a Logistic Regression model using the SMOTE-balanced training data.\n",
    "3. Evaluate the model using a classification report, including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a03afa-99d0-4a8e-883a-e79cfef6168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply SMOTE to balance the classes in the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Step 2: Train a Logistic Regression model using the SMOTE-balanced training data\n",
    "model_smote = LogisticRegression(max_iter=200)\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "\n",
    "# Step 3: Evaluate the model using a classification report, including precision, recall, and F1-score\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "report_smote = classification_report(y_test, y_pred_smote)\n",
    "print(\"Classification Report with SMOTE:\")\n",
    "print(report_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7bae2-a0b6-4b3d-b309-8eb8a355072c",
   "metadata": {},
   "source": [
    "### Task 5: Handling Class Imbalance Using SMOTE Tomek Links\n",
    "\n",
    "1. Apply SMOTE Tomek Links to balance the classes in the training data.\n",
    "2. Train a Logistic Regression model using the SMOTE Tomek Links-balanced training data.\n",
    "3. Evaluate the model using a classification report, including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b2843-895a-46ae-ad62-21a5c78331e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply SMOTE Tomek Links to balance the classes in the training data\n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_tomek, y_tomek = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 2: Train a Logistic Regression model using the SMOTE Tomek Links-balanced training data\n",
    "model_tomek = LogisticRegression(max_iter=2000)\n",
    "model_tomek.fit(X_tomek, y_tomek)\n",
    "\n",
    "# Step 3: Evaluate the model using a classification report, including precision, recall, and F1-score\n",
    "y_pred_tomek = model_tomek.predict(X_test)\n",
    "report_tomek = classification_report(y_test, y_pred_tomek)\n",
    "print(\"Classification Report with SMOTE Tomek Links:\")\n",
    "print(report_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343631d-00cc-4221-a9a4-d15500125d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4e60e-897d-4a59-a0b7-61bd860afcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0819f2b-80b5-41e1-baa1-16336bf84d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementing SVM from Scratch using Submanifold Minimization Algorithm\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate  # Step size for gradient descent\n",
    "        self.lambda_param = lambda_param  # Regularization parameter\n",
    "        self.n_iters = n_iters  # Number of iterations\n",
    "        self.w = None  # Weights\n",
    "        self.b = None  # Bias\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the SVM model using gradient descent with hinge loss.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Gradient descent optimization\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    # No margin violation: update weights with regularization only\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    # Margin violation: apply hinge loss gradient\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y[idx]))\n",
    "                    self.b -= self.learning_rate * y[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.w) - self.b\n",
    "        return np.sign(linear_output)\n",
    "\n",
    "# Define dataset from the image\n",
    "X_pos = np.array([[2.0, 2.2], [2.7, 2.5], [2.3, 2.0], [3.1, 2.3], [2.5, 2.4], [2.8, 2.7]])\n",
    "y_pos = np.ones((X_pos.shape[0],))  # Labels for class +1\n",
    "\n",
    "X_neg = np.array([[1.6, 1.5], [2.0, 1.9], [2.1, 1.8], [1.7, 1.6], [1.8, 1.7], [2.0, 1.6]])\n",
    "y_neg = -np.ones((X_neg.shape[0],))  # Labels for class -1\n",
    "\n",
    "# Combine dataset\n",
    "X = np.vstack((X_pos, X_neg))\n",
    "y = np.hstack((y_pos, y_neg))\n",
    "\n",
    "# Train SVM model\n",
    "svm = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm.predict(X)\n",
    "\n",
    "# Compute evaluation metrics manually\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == -1) & (y_pred == 1))\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == -1))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "# Compute and store results\n",
    "accuracy_val = accuracy(y, y_pred)\n",
    "recall_val = recall(y, y_pred)\n",
    "f1_val = f1_score(y, y_pred)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Recall\", \"F1 Score\"],\n",
    "    \"Value\": [accuracy_val, recall_val, f1_val]\n",
    "})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"SVM Evaluation Metrics (No Sklearn)\", dataframe=metrics_df)\n",
    "\n",
    "# Plot the dataset and decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pos[:, 0], X_pos[:, 1], color='blue', label=\"Class +1\")\n",
    "plt.scatter(X_neg[:, 0], X_neg[:, 1], color='red', label=\"Class -1\")\n",
    "\n",
    "# Compute decision boundary\n",
    "x_vals = np.linspace(1.5, 3.5, 100)\n",
    "y_vals = -(svm.w[0] * x_vals + svm.b) / svm.w[1]\n",
    "plt.plot(x_vals, y_vals, color='black', linestyle='--', label=\"Decision Boundary\")\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"SVM Decision Boundary (No Sklearn)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Re-import necessary libraries after execution state reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "# Define the dataset from the provided image\n",
    "X_pos = np.array([[2.0, 2.2], [2.7, 2.5], [2.3, 2.0], [3.1, 2.3], [2.5, 2.4], [2.8, 2.7]])\n",
    "y_pos = np.ones((X_pos.shape[0],))  # Labels for class +1\n",
    "\n",
    "X_neg = np.array([[1.6, 1.5], [2.0, 1.9], [2.1, 1.8], [1.7, 1.6], [1.8, 1.7], [2.0, 1.6]])\n",
    "y_neg = -np.ones((X_neg.shape[0],))  # Labels for class -1\n",
    "\n",
    "# Combine the dataset\n",
    "X = np.vstack((X_pos, X_neg))\n",
    "y = np.hstack((y_pos, y_neg))\n",
    "\n",
    "# Train an SVM classifier using a linear kernel\n",
    "svm_model = SVC(kernel='linear')  \n",
    "svm_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "\n",
    "# Display results\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"F1 Score\", \"Recall\"],\n",
    "    \"Value\": [accuracy, f1, recall]\n",
    "})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"SVM Evaluation Metrics\", dataframe=metrics_df)\n",
    "\n",
    "# Plot the dataset and decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pos[:, 0], X_pos[:, 1], color='blue', label=\"Class +1\")\n",
    "plt.scatter(X_neg[:, 0], X_neg[:, 1], color='red', label=\"Class -1\")\n",
    "\n",
    "# Plot decision boundary\n",
    "w = svm_model.coef_[0]\n",
    "b = svm_model.intercept_[0]\n",
    "x_vals = np.linspace(1.5, 3.5, 100)\n",
    "y_vals = -(w[0] * x_vals + b) / w[1]\n",
    "plt.plot(x_vals, y_vals, color='black', linestyle='--', label=\"Decision Boundary\")\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"SVM Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### Using Algorithm in Note\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SubmanifoldSVM:\n",
    "    def __init__(self, C=1.0, tol=1e-5, max_iter=1000):\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.tol = tol  # Tolerance for stopping criteria\n",
    "        self.max_iter = max_iter  # Maximum iterations\n",
    "        self.w = None  # Weight vector\n",
    "        self.b = None  # Bias term\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train SVM using the Submanifold Minimization Algorithm.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Define matrices for quadratic programming (QP)\n",
    "        C_mat = np.eye(n_features + 1)  # Identity matrix for regularization\n",
    "        C_mat[-1, -1] = 0  # No regularization for bias term\n",
    "        c_vec = np.zeros(n_features + 1)  # Zero initialization\n",
    "\n",
    "        # Define inequality constraints: y * (Xw + b) >= 1\n",
    "        A = np.hstack((X * y[:, np.newaxis], y[:, np.newaxis]))  # Constraint matrix\n",
    "        a = np.ones(n_samples)  # Constraint vector\n",
    "\n",
    "        # Define bound constraints: 0 <= alpha <= C\n",
    "        B = np.vstack((np.eye(n_samples), -np.eye(n_samples)))  # Bounds for Lagrange multipliers\n",
    "        b = np.hstack((self.C * np.ones(n_samples), np.zeros(n_samples)))  # Limits for alphas\n",
    "\n",
    "        # Initialize w and b\n",
    "        x = np.zeros(n_features + 1)  # Initial guess for [w, b]\n",
    "\n",
    "        # Step 1: Initialize active constraints set\n",
    "        K0 = np.where((B @ x - b) == 0)[0]\n",
    "\n",
    "        # Iterative Optimization Process\n",
    "        for _ in range(self.max_iter):\n",
    "            while True:\n",
    "                # Step 2: Solve quadratic problem with active constraints\n",
    "                A_tilde = np.vstack((A, B[K0]))  # Active constraints\n",
    "                a_tilde = np.hstack((a, b[K0]))\n",
    "\n",
    "                # Solve for x_star using normal equation\n",
    "                QP_mat = np.block([[C_mat, A_tilde.T], [A_tilde, np.zeros((A_tilde.shape[0], A_tilde.shape[0]))]])\n",
    "                rhs = np.hstack((c_vec, a_tilde))\n",
    "\n",
    "                try:\n",
    "                    solution = np.linalg.solve(QP_mat, rhs)\n",
    "                    x_star, nu_star = solution[: n_features + 1], solution[n_features + 1 :]\n",
    "                except np.linalg.LinAlgError:\n",
    "                    break  # If singular, break\n",
    "\n",
    "                # Step 3: Check convergence\n",
    "                if np.dot(x_star, x_star) >= np.dot(x, x):\n",
    "                    break\n",
    "\n",
    "                # Step 4: Compute step size\n",
    "                step_size = np.max([mu for mu in np.linspace(0, 1, 100) if np.all(B @ (x + mu * (x_star - x)) <= b)])\n",
    "                x = x + step_size * (x_star - x)\n",
    "\n",
    "                # Step 5: Update active constraint set\n",
    "                K0 = np.where((B @ x - b) == 0)[0]\n",
    "\n",
    "            # Step 6: Remove negative constraints\n",
    "            if np.all(nu_star >= 0):\n",
    "                break\n",
    "\n",
    "            K0 = np.delete(K0, np.where(nu_star < 0)[0])\n",
    "\n",
    "        # Extract weights and bias\n",
    "        self.w, self.b = x[:-1], x[-1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using trained SVM model.\n",
    "        \"\"\"\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "# Define dataset from the image\n",
    "X_pos = np.array([[2.0, 2.2], [2.7, 2.5], [2.3, 2.0], [3.1, 2.3], [2.5, 2.4], [2.8, 2.7]])\n",
    "y_pos = np.ones((X_pos.shape[0],))  # Labels for class +1\n",
    "\n",
    "X_neg = np.array([[1.6, 1.5], [2.0, 1.9], [2.1, 1.8], [1.7, 1.6], [1.8, 1.7], [2.0, 1.6]])\n",
    "y_neg = -np.ones((X_neg.shape[0],))  # Labels for class -1\n",
    "\n",
    "# Combine dataset\n",
    "X = np.vstack((X_pos, X_neg))\n",
    "y = np.hstack((y_pos, y_neg))\n",
    "\n",
    "# Train SVM model using Submanifold Minimization Algorithm\n",
    "svm = SubmanifoldSVM(C=1.0, max_iter=1000)\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm.predict(X)\n",
    "\n",
    "# Compute evaluation metrics manually\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == -1) & (y_pred == 1))\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == -1))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "# Compute and store results\n",
    "accuracy_val = accuracy(y, y_pred)\n",
    "recall_val = recall(y, y_pred)\n",
    "f1_val = f1_score(y, y_pred)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Recall\", \"F1 Score\"],\n",
    "    \"Value\": [accuracy_val, recall_val, f1_val]\n",
    "})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"SVM Evaluation Metrics (Submanifold Minimization)\", dataframe=metrics_df)\n",
    "\n",
    "# Plot the dataset and decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pos[:, 0], X_pos[:, 1], color='blue', label=\"Class +1\")\n",
    "plt.scatter(X_neg[:, 0], X_neg[:, 1], color='red', label=\"Class -1\")\n",
    "\n",
    "# Compute decision boundary\n",
    "x_vals = np.linspace(1.5, 3.5, 100)\n",
    "y_vals = -(svm.w[0] * x_vals + svm.b) / svm.w[1]\n",
    "plt.plot(x_vals, y_vals, color='black', linestyle='--', label=\"Decision Boundary\")\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"SVM Decision Boundary (Submanifold Minimization Algorithm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1346a8b-6324-454d-84a4-40da9d5f550d",
   "metadata": {},
   "source": [
    "In this part, you will be using the credit card fraud detection dataset from https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud to train and test a Support Vector Machine (SVM) classifier. Your task\n",
    "is to:\n",
    "\n",
    "1. Download the data and split the dataset into training and testing sets (80-20 split) in a stratified manner to take care of the class imbalance. You need to code the stratified splitting function from scratch. *sklearn is not allowed for this part*\n",
    "1. Implement the basic Pegasos Algorithm from the paper https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf. This is in page 5, Fig 1.\n",
    "1. Implement the mini-batch Pegasos algorithm from the paper https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf. Do not forget the projection step. This is in page 6, Fig 2.\n",
    "1. Implement the dual coordinate descent method for SVM’s from the paper https://icml.cc/Conferences/2008/papers/166.pdf. This is Algorithm 1 in the paper.\n",
    "1. Report a final accuracy on the test set for all 3 approches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e372b-0926-47de-877a-1a3c36887e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "\n",
    "def StratifiedSplit(X, y, test_size=0.3):\n",
    "    unique_classes = np.unique(y)\n",
    "    \n",
    "    train_indices, test_indices = [], []\n",
    "    \n",
    "    for class_label in unique_classes:\n",
    "        # Find indices of samples with the current class label\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        # Calculate the number of samples for the test set\n",
    "        num_test_samples = int(len(class_indices) * test_size)\n",
    "        \n",
    "        # Split indices into train and test sets\n",
    "        train_indices.extend(class_indices[num_test_samples:])\n",
    "        test_indices.extend(class_indices[:num_test_samples])\n",
    "    \n",
    "    # Shuffle the indices to randomize the order\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    # Create the train and test sets based on the indices\n",
    "    X_train, X_test = X[train_indices,:], X[test_indices,:]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# reading the file\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "# separating into X and Y\n",
    "X = df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "y = df.iloc[:,-1].values\n",
    "# make the y labels as -1,1 instead of 0,1\n",
    "y = np.where(y>0,y,-1)\n",
    "X_train, X_test, y_train, y_test = StratifiedSplit(X.values, y, test_size=0.2)\n",
    "\n",
    "class Pegasos:\n",
    "    def __init__ (self, lamda, k, projection):\n",
    "        self.lamda = lamda #lambda value\n",
    "        self.k = k #number of observations to be used\n",
    "        self.projection = projection #for projections\n",
    "        \n",
    "    def gradient(self, p): #to calculate the gradient\n",
    "        return np.where(p<1,1,0)\n",
    "    \n",
    "    def fit(self, x, y, n_iters=1500):\n",
    "        m, n = x.shape\n",
    "        self.W = np.zeros(n)\n",
    "        for t in range(1,n_iters+1): #iterate till max_iters\n",
    "            #pick a random instance\n",
    "            idx = np.random.choice(range(m), self.k, replace=False)\n",
    "            lr = 1/(self.lamda*t) #get the learning rate\n",
    "            x_i = x[idx] #get x_i\n",
    "            y_i = y[idx] #get y_i\n",
    "            prod = y_i * (x_i@self.W) #obtain the product\n",
    "            #update the weights\n",
    "            self.W = (1-lr*self.lamda)*self.W + \\\n",
    "            (lr/self.k)*(np.sum(np.multiply(y_i.reshape(-1,1),x_i)*self.gradient(prod).reshape(-1,1),axis=0))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        #transform the inputs using the weight vector\n",
    "        p = x@self.W.reshape(-1,1)\n",
    "        return np.sign(p) #the sign function outputs the class\n",
    "\n",
    "\n",
    "class SVMDC:\n",
    "    def __init__ (self, C, mode = \"L1\", tol=1e-3):\n",
    "        self.C = C #C value\n",
    "        self.mode = mode\n",
    "        self.tol = tol #tolerance value to break out of the loop\n",
    "    \n",
    "    def partial_gradient(self,G,a,U): #to calculate the partial gradient\n",
    "        if a == 0:\n",
    "            return min(G,0)\n",
    "        elif a == U:\n",
    "            return max(G,0)\n",
    "        elif (a>0) and (a<U):\n",
    "            return G\n",
    "    \n",
    "    def fit(self, X, y,iters=100):\n",
    "        m, n = X.shape\n",
    "        self.w = 0 #weight matrix\n",
    "        \n",
    "        #SVMDC can be done in L1 and L2 modes\n",
    "        if self.mode == \"L1\":\n",
    "            Dii = 0\n",
    "            U=self.C\n",
    "        else:\n",
    "            Dii = 1/(2*self.C)\n",
    "            U=np.inf\n",
    "        \n",
    "        #to get the langrangian multipliers\n",
    "        alpha = np.zeros(m)\n",
    "        self.w = np.zeros(shape=(n)) #initialize the weight matrix\n",
    "        Qii = np.sum(X**2, 1) + Dii #calculate Qii\n",
    "        for t in range(iters): #iterate till max_iters\n",
    "            err = 0 #calculate error to break the loop\n",
    "            for i in range(m): #iterate over each instance\n",
    "                Qhat = Qii[i] #get Q_bar\n",
    "                G = np.multiply(np.dot(self.w,X[i,:]),y[i]) - 1 + Dii * alpha[i] #gradient of the objective function\n",
    "                PG = self.partial_gradient(G,alpha[i],U) #partial gradient of the objective function\n",
    "                if np.abs(G) > err: #to keep updating the error term\n",
    "                    err = np.abs(G)\n",
    "                \n",
    "                #to find optimal solution\n",
    "                if np.abs(G) > 0: \n",
    "                    alpha_new = min(max(alpha[i]-G/Qhat,0),U)\n",
    "                    self.w = self.w+(np.multiply((alpha_new - alpha[i])* y[i] ,X[i,:]))\n",
    "                    alpha[i] = alpha_new\n",
    "            \n",
    "            #stop iterating once the error fall below tolerance        \n",
    "            if err<self.tol:\n",
    "                break\n",
    "        \n",
    "    def predict(self, x):\n",
    "        #project the points using the weight matrix\n",
    "        p = x@self.w.reshape(-1,1)\n",
    "        return np.sign(p) #the sign function tells the class which the object belong to\n",
    "\n",
    "\n",
    "\n",
    "# Pegasos Basic\n",
    "peg_basic = Pegasos(0.01,1,projection=False)\n",
    "peg_basic.fit(X_train,y_train)\n",
    "\n",
    "# Test the model\n",
    "print(f'Accuracy of Basic Pegasos is {accuracy_score(y_test,peg_basic.predict(X_test))} and ROC AUC Score is {roc_auc_score(y_test,peg_basic.predict(X_test))}')\n",
    "\n",
    "# Pegasos Batch\n",
    "peg_batch = Pegasos(0.01,10,projection=False)\n",
    "peg_batch.fit(X_train,y_train)\n",
    "# Test the model\n",
    "print(f'Accuracy of Batch Pegasos is {accuracy_score(y_test,peg_batch.predict(X_test))} and ROC AUC Score is {roc_auc_score(y_test,peg_batch.predict(X_test))}')\n",
    "\n",
    "\n",
    "# Dual Coordinate Descent\n",
    "svm = SVMDC(0.1)\n",
    "svm.fit(X_train,y_train)\n",
    "# Test the model\n",
    "print(f'Accuracy of Dual Coordinate Descent SVM is {accuracy_score(y_test,svm.predict(X_test))} and ROC AUC Score is {roc_auc_score(y_test,svm.predict(X_test))}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd622d36-bf23-4fc2-96dc-cefbc19a85f8",
   "metadata": {},
   "source": [
    "**NOTE:** Here, since we have a huge class imbalance in the dataset, therefore, a better approach would be to use some techniques like Oversampling, Undersampling, and SMOTE as a preprocessing step and then apply the model over the preprocessed dataset. Additionally, Pegasos and CD uses stochastic optimization approach therefore the results obtained after these techniques may differ largely for each run and for the choice of each random state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a60ec-4c89-4d6e-a081-0d8384b175b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38a4ddf4-3db0-4bc0-b2aa-a9614897ef3f",
   "metadata": {},
   "source": [
    "For the below data, fit a SVM model to dataset. Later, transform the features using PCA to 2-D and then perform SVM. Compare the performance between the two approaches. Do not forget to scale the features before applying PCA. (Use Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17637725-d1a2-40cf-8b3b-2359b1e71e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "\n",
    "svm.fit(X_train,y_train)\n",
    "preds = svm.predict(X_test)\n",
    "accuracy_score(y_test,preds)\n",
    "\n",
    "\n",
    "#Now after doing PCA\n",
    "\n",
    "#To scaling of the data\n",
    "scalar = StandardScaler()\n",
    "X_train_scaled = scalar.fit_transform(X_train)\n",
    "X_test_scaled = scalar.transform(X_test)\n",
    "\n",
    "#apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_scaled,y_train)\n",
    "preds = svm.predict(X_test_scaled)\n",
    "accuracy_score(y_test,preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97aa17-b599-46d4-af5b-75d92be20d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d462c1dc-87d6-4f5f-94da-290b30e91999",
   "metadata": {},
   "source": [
    "For the below dataset. Try to build an ensemble model using logistic regression. You have to select subset of features and instances and fit a logistic regression model. You can take number of logistic regression models to be 10. To get final output, perform soft voting (take the average of the probabilities and then apply the thereshold)\n",
    "\n",
    "Note: Sklearn is allowed for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7c2f7-dbb7-41df-9986-73a20a0815d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "column_names = [\n",
    "    \"pregnancies\", \"glucose\", \"blood_pressure\", \"skin_thickness\", \"insulin\",\n",
    "    \"bmi\", \"diabetes_pedigree_function\", \"age\", \"outcome\"\n",
    "]\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "diabetes_data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "X = diabetes_data.iloc[:,:-1].values\n",
    "y = diabetes_data.iloc[:,-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Since we have 614 instances let's choose each learner model is trained using 200 instances and 4 features\n",
    "\n",
    "models = [] \n",
    "selected_features = [] #to store features used for each learner model\n",
    "\n",
    "for i in range(10):\n",
    "    #Create a random subset of features\n",
    "    subset_features = np.random.choice(range(X_train.shape[1]), size=4, replace=False) #to retrieve features for testing\n",
    "    X_subset_features = X_train[:, subset_features]\n",
    "    selected_features.append(subset_features)\n",
    "\n",
    "    #Create a random subset of instances\n",
    "    subset_instances = np.random.choice(range(X_train.shape[0]), size=200, replace=True)\n",
    "    X_subset = X_subset_features[subset_instances, :]\n",
    "    y_subset = y_train[subset_instances]\n",
    "\n",
    "    #Create and fit a logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_subset, y_subset)\n",
    "\n",
    "    #Append the trained model to the list\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "#now to create soft voting module\n",
    "\n",
    "#make prediction on test set\n",
    "probabilities = np.array([model.predict_proba(X_test[:, features]) for model, features in zip(models, selected_features)])\n",
    "\n",
    "#calculate the mean probabilities\n",
    "average_probabilities = np.mean(probabilities, axis=0)\n",
    "\n",
    "#make final decision based on the final averaged probabilities\n",
    "final_prediction = np.argmax(average_probabilities, axis=1)\n",
    "\n",
    "#To get accuracy of the ensemble model\n",
    "accuracy_score(y_test, final_prediction)\n",
    "\n",
    "#What if  we had only used a single Log REg model instead of ensemble\n",
    "single_log = LogisticRegression(max_iter=1000)\n",
    "single_log.fit(X_train, y_train)\n",
    "single_pred = single_log.predict(X_test)\n",
    "accuracy_score(y_test, single_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349da7ea-ec38-4c77-95d2-188544416c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f40261-898e-4bf9-b47e-ad3e4eba2049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69596f29-7b88-498b-a321-9e5afb415df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_pos = np.array([[2.0, 2.2],\n",
    "#                   [2.7, 2.5],\n",
    "#                   [2.3, 2.0],\n",
    "#                   [3.1, 2.3],\n",
    "#                   [2.5, 2.4],\n",
    "#                   [2.8, 2.7]])\n",
    "\n",
    "# y_pos = np.ones(X_pos.shape[0])\n",
    "\n",
    "# X_neg = np.array([[1.6, 1.5],\n",
    "#                   [2.0, 1.9],\n",
    "#                   [2.1, 1.8],\n",
    "#                   [1.7, 1.6],\n",
    "#                   [1.8, 1.7],\n",
    "#                   [2.0, 1.6]])\n",
    "\n",
    "# y_neg = -1 * np.ones(X_neg.shape[0])\n",
    "\n",
    "# # Combine positive and negative samples\n",
    "# X = np.vstack((X_pos, X_neg))\n",
    "# y = np.hstack((y_pos, y_neg))\n",
    "\n",
    "# # Helper function for inequality-constrained submanifold minimization\n",
    "# def minimize_ineq_cstr_submanifold(C, c, A, a, B, b, x_init, max_iterations=1000, tol=1e-6):\n",
    "#     \"\"\"\n",
    "#     Implements the inequality-constrained submanifold minimization algorithm.\n",
    "\n",
    "#     Parameters:\n",
    "#         C: Quadratic term in the objective function (N x N matrix)\n",
    "#         c: Linear term in the objective function (N-dimensional vector)\n",
    "#         A: Equality constraint matrix (M x N matrix)\n",
    "#         a: Equality constraint vector (M-dimensional vector)\n",
    "#         B: Inequality constraint matrix (K x N matrix)\n",
    "#         b: Inequality constraint vector (K-dimensional vector)\n",
    "#         x_init: Initial guess for x (N-dimensional vector)\n",
    "#         max_iterations: Maximum number of iterations\n",
    "#         tol: Convergence tolerance\n",
    "\n",
    "#     Returns:\n",
    "#         x: Optimized variable vector\n",
    "#     \"\"\"\n",
    "#     x = x_init\n",
    "#     K0 = {k for k in range(B.shape[0]) if np.isclose((B @ x - b)[k], 0)}\n",
    "\n",
    "#     while True:\n",
    "#         while True:\n",
    "#             A_tilde = np.vstack([A, B[list(K0), :]])\n",
    "#             a_tilde = np.hstack([a, b[list(K0)]])\n",
    "\n",
    "#             # Solve the linear system\n",
    "#             system_matrix = np.block([[C, A_tilde.T], [A_tilde, np.zeros((A_tilde.shape[0], A_tilde.shape[0]))]])\n",
    "#             rhs = np.hstack([c, a_tilde])\n",
    "#             solution = np.linalg.solve(system_matrix, rhs)\n",
    "#             x_star, nu_star = solution[:x.shape[0]], solution[x.shape[0]:]\n",
    "\n",
    "#             # Check for improvement in the objective function\n",
    "#             if (c @ x_star + 0.5 * x_star @ C @ x_star) >= (c @ x + 0.5 * x @ C @ x):\n",
    "#                 break\n",
    "\n",
    "#             # Line search\n",
    "#             direction = x_star - x\n",
    "#             mu = max([mu for mu in np.linspace(0, 1, 100) if np.all(B @ (x + mu * direction) - b <= 0)])\n",
    "#             x = x + mu * direction\n",
    "#             K0 = {k for k in range(B.shape[0]) if np.isclose((B @ x - b)[k], 0)}\n",
    "\n",
    "#         # Check optimality of dual variables\n",
    "#         if np.all(nu_star >= 0):\n",
    "#             break\n",
    "\n",
    "#         # Remove violating constraint from active set\n",
    "#         violating_constraints = [k for k in K0 if nu_star[k] < 0]\n",
    "#         if violating_constraints:\n",
    "#             K0.remove(violating_constraints[0])\n",
    "\n",
    "#     return x\n",
    "\n",
    "# # SVM training using the submanifold minimization algorithm\n",
    "# def learn_svm(D_train, gamma, K_func):\n",
    "#     \"\"\"\n",
    "#     Trains an SVM using the dataset and the submanifold minimization algorithm.\n",
    "\n",
    "#     Parameters:\n",
    "#         D_train: Training data as a list of (x_i, y_i) tuples\n",
    "#         gamma: Regularization parameter\n",
    "#         K_func: Kernel function\n",
    "\n",
    "#     Returns:\n",
    "#         beta_0: Bias term\n",
    "#         alpha: Dual variables\n",
    "#     \"\"\"\n",
    "#     X, y = zip(*D_train)\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "#     N = len(y)\n",
    "\n",
    "#     # Construct components for the optimization problem\n",
    "#     K = np.array([[K_func(x_i, x_j) for x_j in X] for x_i in X])\n",
    "#     C = np.outer(y, y) * K\n",
    "#     c = np.ones(N)\n",
    "#     A = y.reshape(1, -1)\n",
    "#     a = np.array([0])\n",
    "#     B = np.vstack([-np.eye(N), np.eye(N)])\n",
    "#     b = np.hstack([np.zeros(N), gamma * np.ones(N)])\n",
    "\n",
    "#     # Initial guess for alpha\n",
    "#     N_pos = sum(y == 1)\n",
    "#     N_neg = N - N_pos\n",
    "#     alpha_init = np.where(y == 1, gamma / N_pos, gamma / N_neg)\n",
    "\n",
    "#     # Minimize the constrained problem\n",
    "#     alpha = minimize_ineq_cstr_submanifold(C, c, A, a, B, b, alpha_init)\n",
    "\n",
    "#     # Compute the bias term\n",
    "#     support_indices = np.where(alpha > 1e-6)[0]\n",
    "#     beta_0 = np.mean([y[n] - sum(alpha[m] * y[m] * K[m, n] for m in support_indices) for n in support_indices])\n",
    "\n",
    "#     return beta_0, alpha\n",
    "\n",
    "# # Example usage\n",
    "# D_train = [(X[i], y[i]) for i in range(len(y))]\n",
    "\n",
    "# def linear_kernel(x1, x2):\n",
    "#     return np.dot(x1, x2)\n",
    "\n",
    "# gamma = 1.0\n",
    "# beta_0, alpha = learn_svm(D_train, gamma, linear_kernel)\n",
    "\n",
    "# print(\"Bias (beta_0):\", beta_0)\n",
    "# print(\"Alpha coefficients:\", alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af355-fb0d-499a-98ea-b4d2460d95aa",
   "metadata": {},
   "source": [
    "### 2. Imbalanced Classification with Sampling Techniques and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42d53f7-f1a7-4827-8901-36b7e319361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f4c6e6a-e480-44f4-9b66-e45a23ba584a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cdd72-9e90-4e23-b284-46cf91841c4d",
   "metadata": {},
   "source": [
    "#### a.) Applying Smote Oversampling and Rnadom Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc40b39-bace-46d1-927a-f44462acfae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((454902, 30), (788, 30))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# SMOTE Oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#Random Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "X_train_smote.shape, X_train_under.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1458d-2fb6-4f37-806a-0fd97271c3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b68f67a7-b373-4d99-8aaa-cafbc42d186d",
   "metadata": {},
   "source": [
    "### b.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cebdf256-37f7-48a4-be40-24c3189e5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "# Custom MLP Class\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation_fn = nn.ReLU()\n",
    "\n",
    "        # Hidden layers\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(prev_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation_fn(layer(x))\n",
    "        return torch.sigmoid(self.output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9aba1ed-1346-4336-a7e7-c4817792fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "def prepare_dataloader(X, y, batch_size=64):\n",
    "    dataset = TensorDataset(torch.tensor(X.values, dtype=torch.float32),\n",
    "                            torch.tensor(y.values, dtype=torch.float32).unsqueeze(1))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Model Initialization\n",
    "input_size = X_train.shape[1]\n",
    "hidden_layers = [64, 32, 16]\n",
    "model = CustomMLP(input_size, hidden_layers)\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38729569-1003-4b88-b4a6-f9bc61a9f6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "772df437-217d-4fba-b9d1-a1869e705713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Original Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 1/50 [00:30<24:55, 30.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.1985 - Test Loss: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|██▏       | 11/50 [04:18<14:36, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Train Loss: 0.1729 - Test Loss: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|████▏     | 21/50 [2:30:00<59:04, 122.23s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 - Train Loss: 0.1729 - Test Loss: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|██████▏   | 31/50 [2:33:02<06:37, 20.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 - Train Loss: 0.1729 - Test Loss: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  82%|████████▏ | 41/50 [2:36:10<03:02, 20.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 - Train Loss: 0.1729 - Test Loss: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 50/50 [2:39:32<00:00, 191.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on SMOTE Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 1/50 [00:31<25:38, 31.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 49.6609 - Test Loss: 99.7009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|██▏       | 11/50 [09:49<50:57, 78.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Train Loss: 49.8580 - Test Loss: 99.6393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|████▏     | 21/50 [20:27<28:35, 59.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 - Train Loss: 49.8167 - Test Loss: 0.1756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|██████▏   | 31/50 [36:28<45:31, 143.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 - Train Loss: 49.7608 - Test Loss: 0.1756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  82%|████████▏ | 41/50 [45:40<08:54, 59.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 - Train Loss: 49.2281 - Test Loss: 98.6588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 50/50 [1:00:09<00:00, 72.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Undersampled Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 1/50 [00:02<02:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 47.1822 - Test Loss: 99.1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|██▏       | 11/50 [00:32<01:53,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Train Loss: 49.7696 - Test Loss: 99.1953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|████▏     | 21/50 [01:14<02:16,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 - Train Loss: 49.5472 - Test Loss: 98.9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|██████▏   | 31/50 [01:45<00:56,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 - Train Loss: 49.8874 - Test Loss: 99.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  82%|████████▏ | 41/50 [04:24<00:52,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 - Train Loss: 49.8861 - Test Loss: 99.5444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 50/50 [04:48<00:00,  5.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# Training and test Functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "def test_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    return test_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "datasets = {\n",
    "    \"Original\": (X_train, y_train),\n",
    "    \"SMOTE\": (X_train_smote, y_train_smote),\n",
    "    \"Undersampled\": (X_train_under, y_train_under)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, (X_data, y_data) in datasets.items():\n",
    "    print(f\"\\nTraining on {name} Dataset...\")\n",
    "    train_loader = prepare_dataloader(X_data, y_data, batch_size)\n",
    "    test_loader = prepare_dataloader(X_test, y_test, batch_size)\n",
    "    \n",
    "    model = CustomMLP(input_size, hidden_layers).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in trange(n_epochs, desc=\"Epochs\"):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        test_loss = test_epoch(model, test_loader, criterion, device)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.4f} - Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    results[name] = {\"train_losses\": train_losses, \"test_losses\": test_losses}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee5bb2-c4e0-4fe6-aaa6-d1dd11002754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d27f2415-730d-459b-9a41-937cdd7ff0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model trained on Original dataset...\n",
      "Evaluating model trained on SMOTE dataset...\n",
      "Evaluating model trained on Undersampled dataset...\n",
      "\n",
      "Metrics for Original Dataset:\n",
      "Accuracy: 0.0018\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.0034\n",
      "\n",
      "Metrics for SMOTE Dataset:\n",
      "Accuracy: 0.0018\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.0034\n",
      "\n",
      "Metrics for Undersampled Dataset:\n",
      "Accuracy: 0.0018\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.0034\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X_test, y_test, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        \n",
    "        predictions = model(X_test_tensor).cpu().numpy()\n",
    "        predictions = (predictions >= 0.5).astype(int)  \n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        f1 = f1_score(y_test, predictions)\n",
    "        \n",
    "        return accuracy, recall, f1\n",
    "\n",
    "# Evaluating the models for each dataset\n",
    "metrics = {}\n",
    "for name, (X_data, y_data) in datasets.items():\n",
    "    print(f\"Evaluating model trained on {name} dataset...\")\n",
    "    accuracy, recall, f1 = evaluate_model(model, X_test, y_test, device)\n",
    "    metrics[name] = {\"Accuracy\": accuracy, \"Recall\": recall, \"F1-Score\": f1}\n",
    "\n",
    "# Display metrics\n",
    "for dataset, values in metrics.items():\n",
    "    print(f\"\\nMetrics for {dataset} Dataset:\")\n",
    "    for metric, value in values.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e445f7-52a8-45e4-9afc-cd4010c65be2",
   "metadata": {},
   "source": [
    "### Comparing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead9d82-c92f-4740-afc2-315f525daff4",
   "metadata": {},
   "source": [
    "\n",
    " - Original Dataset: Has less loss than the sampled datasets\n",
    " - SMOTE Oversamplinga: low accuracy\n",
    " - Random Undersampling: low accuracy\n",
    "\n",
    "\n",
    "TRADEOFFS\n",
    " - Accuracy decreasees with SMOTE and undersampling due to changes in class distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89625e42-2d0b-484e-899c-fea7a5077f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
