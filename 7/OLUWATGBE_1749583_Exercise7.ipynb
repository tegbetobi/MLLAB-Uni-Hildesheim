{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca5d02-512f-466b-9449-5b1c2cdc3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e5f65-c1bb-414b-8da3-0c57f0f6750f",
   "metadata": {},
   "source": [
    "### 1. SVM with Submanifold Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b04bbbe-5ff6-4dde-8345-420aecd7d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Map labels to +1 and -1\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Submanifold Minimization Algorithm\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    # Update weights for correctly classified points\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    # Update weights and bias for misclassified points\n",
    "                    self.w -= self.lr * (\n",
    "                        2 * self.lambda_param * self.w - np.dot(x_i, y_[idx])\n",
    "                    )\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c00f640-8bd1-4cb1-a645-542f6a6b3e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.35074838 0.51297922]\n",
      "Bias: 1.313999999999966\n",
      "Predictions: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_pos = np.array([[2.0, 2.2], [2.7, 2.5], [2.3, 2.0], [3.1, 2.3], [2.5, 2.4], [2.8, 2.7]])\n",
    "y_pos = np.ones(len(X_pos))\n",
    "\n",
    "X_neg = np.array([[1.6, 1.5], [2.0, 1.9], [2.1, 1.8], [1.7, 1.6], [1.8, 1.7], [2.0, 1.6]])\n",
    "y_neg = -np.ones(len(X_neg))\n",
    "\n",
    "# Combining the  positive and negative classes\n",
    "X = np.vstack((X_pos, X_neg))\n",
    "y = np.hstack((y_pos, y_neg))\n",
    "\n",
    "clf = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Model parameters\n",
    "print(\"Weights:\", clf.w)\n",
    "print(\"Bias:\", clf.b)\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(X)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee86c68-cdd6-496b-87cf-7197b36bf713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80bc247b-d049-45ff-934c-f96e834e7a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "F1 Score: 0.6666666666666666\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y, predictions)\n",
    "f1 = f1_score(y, predictions)\n",
    "recall = recall_score(y, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed03dd-860f-4be3-beac-cfcb561fa979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51870a32-9e5f-4ce9-bf80-85da7ce2c577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d81f3768-ef69-4823-9cee-8947d3287ba7",
   "metadata": {},
   "source": [
    "### 2. Imbalanced Classification with Sampling Techniques and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439996fe-447c-4d7f-838b-6b81c8f4ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8c9b7f6-ca8c-4cfa-999c-cf207e3d91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8125471c-4e1c-49ab-8134-f776577ba935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845, 30)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e558167-671e-427f-8731-f10483f7fcb1",
   "metadata": {},
   "source": [
    "#### a.) Applying Smote Oversampling and Rnadom Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f585b2f6-30fb-4d32-9647-7e677fad69d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((454902, 30), (788, 30))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# SMOTE Oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#Random Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "X_train_smote.shape, X_train_under.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed860196-88de-4150-8936-0f6afa7a45d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e9a71a7-ea85-4e2a-8843-04413c7ead36",
   "metadata": {},
   "source": [
    "### b.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a072dd9b-2e26-46aa-bb69-c19f47015009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "# Custom MLP Class\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation_fn = nn.ReLU()\n",
    "\n",
    "        # Hidden layers\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(prev_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation_fn(layer(x))\n",
    "        return torch.sigmoid(self.output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a397133b-95f6-44ca-bdef-edda93e74fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "def prepare_dataloader(X, y, batch_size=64):\n",
    "    dataset = TensorDataset(torch.tensor(X.values, dtype=torch.float32),\n",
    "                            torch.tensor(y.values, dtype=torch.float32).unsqueeze(1))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Model Initialization\n",
    "input_size = X_train.shape[1]\n",
    "hidden_layers = [64, 32, 16]\n",
    "model = CustomMLP(input_size, hidden_layers)\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4030011-3225-438d-81d3-db5ebf0a60ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49e617b1-211c-49c2-8f94-bf8345a3e7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Original Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|‚ñè         | 1/50 [00:17<14:30, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.1729 - Test Loss: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|‚ñç         | 2/50 [00:38<15:13, 19.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(n_epochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 58\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, criterion, optimizer, device)\n\u001b[0;32m     59\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     61\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_epoch(model, test_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[47], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:697\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction\n\u001b[0;32m    699\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3554\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3551\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3552\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight, reduction_enum)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and test Functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "def test_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    return test_loss / len(dataloader.dataset)\n",
    "\n",
    "# Training Loop\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "datasets = {\n",
    "    \"Original\": (X_train, y_train),\n",
    "    \"SMOTE\": (X_train_smote, y_train_smote),\n",
    "    \"Undersampled\": (X_train_under, y_train_under)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, (X_data, y_data) in datasets.items():\n",
    "    print(f\"\\nTraining on {name} Dataset...\")\n",
    "    train_loader = prepare_dataloader(X_data, y_data, batch_size)\n",
    "    test_loader = prepare_dataloader(X_test, y_test, batch_size)\n",
    "    \n",
    "    model = CustomMLP(input_size, hidden_layers).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in trange(n_epochs, desc=\"Epochs\"):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        test_loss = test_epoch(model, test_loader, criterion, device)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.4f} - Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    results[name] = {\"train_losses\": train_losses, \"test_losses\": test_losses}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa559b6-0529-498c-91f2-c4e41c48ba7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd7f94-1dcb-4316-aebb-1db0aae67618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54d80c-4e27-4d1c-9325-d61ea76f5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X_test, y_test, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        \n",
    "        predictions = model(X_test_tensor).cpu().numpy()\n",
    "        predictions = (predictions >= 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "        \n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        f1 = f1_score(y_test, predictions)\n",
    "        \n",
    "        return accuracy, recall, f1\n",
    "\n",
    "# Evaluate models for each dataset\n",
    "metrics = {}\n",
    "for name, (X_data, y_data) in datasets.items():\n",
    "    print(f\"Evaluating model trained on {name} dataset...\")\n",
    "    accuracy, recall, f1 = evaluate_model(model, X_test, y_test, device)\n",
    "    metrics[name] = {\"Accuracy\": accuracy, \"Recall\": recall, \"F1-Score\": f1}\n",
    "\n",
    "# Display metrics\n",
    "for dataset, values in metrics.items():\n",
    "    print(f\"\\nMetrics for {dataset} Dataset:\")\n",
    "    for metric, value in values.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb465119-7e41-412b-a4c5-db9cb9991cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b1408a4-d9a0-412b-a92a-392b71df333c",
   "metadata": {},
   "source": [
    "### Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6e2b9-ef84-478d-bc5a-61b348a79728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " - Original Dataset: High accuracy but has lower probably due to the imbalance.\n",
    " - SMOTE Oversamplinga: Better recall and F1-score.\n",
    " - Random Undersampling: Okay recall and precision but has a high loss of majority-class data\n",
    "\n",
    "\n",
    "TRADEOFFS\n",
    " - Accuracy may decrease with SMOTE and undersampling due to changes in class distributions.\n",
    " - Recall improves with SMOTE but may overfit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
