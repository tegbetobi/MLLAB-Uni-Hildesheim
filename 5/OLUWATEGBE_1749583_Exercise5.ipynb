{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4286c3e-b07e-4316-b26a-0065bfa08699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4cb7d-591b-4ad6-aa38-99e0ccd30fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473dc5d7-6f32-43a7-9836-94bb9e33a100",
   "metadata": {},
   "source": [
    "## 1. KNN Imputation and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5281592-ffd7-4cc7-ba2b-e3ffba8e9c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef020d58-43ec-4689-b605-c23198918036",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4a1738c-0bcd-46fc-957a-7a0783e7c1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2     3     4    5     6     7     8     9    ...   279   280  \\\n",
       "0  1.0  2.0  8.0   7.0   6.0  2.0   8.0   5.0  10.0   3.0  ...  11.0   7.0   \n",
       "1  1.0  8.0  5.0  10.0  11.0  9.0  10.0   7.0  18.0  11.0  ...  13.0   7.0   \n",
       "2  1.0  1.0  9.0   5.0   2.0  5.0   5.0   5.0  13.0   6.0  ...  10.0   5.0   \n",
       "3  1.0  2.0  6.0   6.0   3.0  8.0   6.0   3.0   4.0   3.0  ...   8.0  13.0   \n",
       "4  1.0  8.0  6.0   5.0  11.0  7.0   4.0  10.0   8.0   8.0  ...  19.0  13.0   \n",
       "\n",
       "    281   282   283   284   285   286   287   288  \n",
       "0  12.0   4.0   9.0  13.0   3.0   5.0   5.0   3.0  \n",
       "1   6.0   7.0   1.0   2.0   6.0   7.0   8.0   6.0  \n",
       "2  14.0   9.0  13.0  10.0   9.0   5.0  11.0   6.0  \n",
       "3   9.0  11.0  10.0   4.0   6.0   6.0   3.0  10.0  \n",
       "4  15.0  17.0  10.0   8.0  12.0  16.0  15.0  13.0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('DodgerLoopGame\\DodgerLoopGame_TRAIN.txt',sep=\"\\s+\",header=None)\n",
    "df2 = pd.read_csv('DodgerLoopGame\\DodgerLoopGame_TEST.txt',sep=\"\\s+\",header=None)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "80b6a0ab-f694-413a-8d90-a9385c60547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numberof NaN values in Train Datasets are 65\n",
      "Numberof NaN values in Test Datasets are 272\n"
     ]
    }
   ],
   "source": [
    "NaTrain = df1.isna().sum().sum()\n",
    "NaTest = df2.isna().sum().sum()\n",
    "print('Numberof NaN values in Train Datasets are',NaTrain)\n",
    "print('Numberof NaN values in Test Datasets are',NaTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9282c5-b66f-470a-8678-5d024d3e282e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97fdb9-8960-4f46-bd3c-2b411af3b458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39a74497-825e-41b1-b1a0-6adaee7dbc03",
   "metadata": {},
   "source": [
    "### b. KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f62abafe-5cc7-4ea0-bb18-9dae7d43d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset Number of NaN values after Imputation: 0\n",
      "\n",
      "Test Dataset Number of NaN values after Imputation: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "impute = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Train Dataset\n",
    "df1_imput = impute.fit_transform(df1)\n",
    "df1_imputed = pd.DataFrame(df1_imput, columns=df1.columns)\n",
    "\n",
    "# Test Dataset\n",
    "df2_imput = impute.fit_transform(df2)\n",
    "df2_imputed = pd.DataFrame(df2_imput, columns=df2.columns)\n",
    "\n",
    "print(\"\\nTrain Dataset Number of NaN values after Imputation:\", df1_imputed.isna().sum().sum())\n",
    "print(\"\\nTest Dataset Number of NaN values after Imputation:\", df2_imputed.isna().sum().sum())\n",
    "\n",
    "#mse = mean_squared_error(df2_imput, df2_imputed)\n",
    "#print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "adfe599e-e12e-4784-b31d-123e4d7a138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1, Mean Distance=179.2819\n",
      "K=3, Mean Distance=178.9790\n",
      "K=5, Mean Distance=178.8261\n",
      "K=7, Mean Distance=178.8077\n",
      "K=9, Mean Distance=178.7810\n",
      "Optimal number of neighbors (K): 9\n",
      "Best Mean Distance: 178.7810\n",
      "Optimal K: 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def knn_imputer_grid_search(X_train, X_test, k_values):\n",
    "    best_k = 1\n",
    "    best_mean_distance = float('inf')\n",
    "    \n",
    "    for k in k_values:\n",
    "    \n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        train_imputed = imputer.fit_transform(X_train)\n",
    "        test_imputed = imputer.transform(X_test)\n",
    "        # Checking shapes to see if it aligns or not ..\n",
    "      #  print(\"Shape of imputed values after flattening:\", np.isnan(imputed_values).sum())\n",
    "        \n",
    "       # distances = cdist(original_values_without_nan.reshape(-1, 1), imputed_values.reshape(-1, 1), metric='euclidean')\n",
    "        distances = cdist(train_imputed, test_imputed, metric='euclidean') # calculating the  pairwise Euclidean distance\n",
    "        mean_distance = np.mean(distances) # Getting the mean distances\n",
    "\n",
    "        print(f\"K={k}, Mean Distance={mean_distance:.4f}\")\n",
    "\n",
    "        # Update the best K if current mean distance is lower\n",
    "        if mean_distance < best_mean_distance:\n",
    "            best_mean_distance = mean_distance\n",
    "            best_k = k\n",
    "            \n",
    "    print(f\"Optimal number of neighbors (K): {best_k}\")    # Using the best K to impute both train and test datasets\n",
    "    print(f\"Best Mean Distance: {best_mean_distance:.4f}\")\n",
    "\n",
    "    # Final imputation with the best K\n",
    "    final_imputer = KNNImputer(n_neighbors=best_k)\n",
    "    # Using the best k for the input data\n",
    "    train_imputed = pd.DataFrame(final_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "    # test_imputed = pd.DataFrame(final_imputer.transform(X_test), columns=X_test.columns  # To perform imputation on the test data here\n",
    "\n",
    "    return best_k\n",
    "\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "optimal_k = knn_imputer_grid_search(df1, df2,k_values)\n",
    "print(f\"Optimal K: {optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933433f9-47b6-49e4-ac19-002fb829b7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf138331-e624-4cc4-beb3-70364463ecc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eae01a-e80a-4ca5-905a-0d5217ef88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ### KNN Inputer from scratch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def manhattan_distance(a, b):\n",
    "    \"\"\"Calculate the Manhattan distance between two points.\"\"\"\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n",
    "def knn_impute(data, k):\n",
    "    \"\"\"\n",
    "    Impute missing values in the dataset using KNN.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 2D numpy array with missing values as np.nan.\n",
    "    - k: Number of nearest neighbors to use for imputation.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array with missing values imputed.\n",
    "    \"\"\"\n",
    "    data_imputed = data.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "\n",
    "    for row_idx in range(n_rows):\n",
    "        for col_idx in range(n_cols):\n",
    "            if np.isnan(data[row_idx, col_idx]):\n",
    "                # Find rows with non-missing values for the current column\n",
    "                valid_rows = [i for i in range(n_rows) if not np.isnan(data[i, col_idx])]\n",
    "\n",
    "                # If no valid rows exist, leave the missing value as is\n",
    "                if not valid_rows:\n",
    "                    continue\n",
    "\n",
    "                # Calculate distances to other rows based on non-missing features\n",
    "                distances = []\n",
    "                for valid_row in valid_rows:\n",
    "                    # Only compare non-missing features\n",
    "                    mask = ~np.isnan(data[row_idx, :]) & ~np.isnan(data[valid_row, :])\n",
    "                    if np.any(mask):  # Ensure there is at least one feature to compare\n",
    "                        distance = manhattan_distance(data[row_idx, mask], data[valid_row, mask])\n",
    "                        distances.append((distance, valid_row))\n",
    "\n",
    "                # Sort by distance and take k nearest neighbors\n",
    "                distances.sort(key=lambda x: x[0])\n",
    "                k_nearest = distances[:k]\n",
    "\n",
    "                # Impute the missing value as the mean of the neighbors' values in the current column\n",
    "                k_values = [data[neighbor[1], col_idx] for neighbor in k_nearest]\n",
    "                data_imputed[row_idx, col_idx] = np.mean(k_values)\n",
    "\n",
    "    return data_imputed\n",
    "\n",
    "def main_knn_imputer():\n",
    "    # Generate synthetic dataset with missing values\n",
    "    np.random.seed(42)\n",
    "    data = np.random.rand(10, 5)\n",
    "    data[1, 2] = np.nan\n",
    "    data[3, 0] = np.nan\n",
    "    data[7, 4] = np.nan\n",
    "    print(\"Original Data with Missing Values:\")\n",
    "    print(data)\n",
    "\n",
    "    # Impute missing values using KNN\n",
    "    k = 3\n",
    "    data_imputed = knn_impute(data, k)\n",
    "\n",
    "    print(\"\\nData after KNN Imputation:\")\n",
    "    print(data_imputed)\n",
    "\n",
    "# Run the main function\n",
    "main_knn_imputer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101313d6-53c9-4f82-b139-d092a9f8b1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd0f0ce-b9a8-4daa-b3be-41fce157eb53",
   "metadata": {},
   "source": [
    "### c. KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458da3d5-2855-4739-90b1-9fdf52e27ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "93f6b619-6afd-4886-b543-43a08c4d346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X,y):\n",
    "    # Calculate the split indices\n",
    "    split_train_idx = int(len(data) * 0.7)  # 70% for training\n",
    "    split_temp_idx = int(len(data) * 0.85)  # 85% for training + validation (so 15% remains for testing)\n",
    "    \n",
    "    # Split the data into training, validation, and test sets\n",
    "    train_data = data[:split_train_idx]      # First 70% for training\n",
    "    validation_data = data[split_train_idx:split_temp_idx]  # Next 15% for validation\n",
    "    test_data = data[split_temp_idx:]    # Last 15% for testing\n",
    "\n",
    "\n",
    "    train_data = data[:split_train_idx]      # First 70% for training\n",
    "    validation_data = data[split_train_idx:split_temp_idx]  # Next 15% for validation\n",
    "    test_data = data[split_temp_idx:]\n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "\n",
    "def train_test_split(data, test_size=0.2):\n",
    "   \n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    train_data = data[:split_idx]\n",
    "    test_data = data[split_idx:]\n",
    "    \n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158810b5-4624-4105-8d48-3744cec77c5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0dc9732e-7899-44a9-9df1-1ecd6305bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1, Train Accuracy: 1.0000, Test Accuracy: 0.5100\n",
      "K=3, Train Accuracy: 0.7750, Test Accuracy: 0.4500\n",
      "K=5, Train Accuracy: 0.6875, Test Accuracy: 0.5100\n",
      "K=7, Train Accuracy: 0.6175, Test Accuracy: 0.4800\n",
      "K=9, Train Accuracy: 0.5800, Test Accuracy: 0.5400\n",
      "Optimal K: 9\n",
      "Best Test Accuracy with K=9: 0.5400\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def EuclideanDistance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "\n",
    "def y_prediction(k_neighbors, y_train):\n",
    "    y_pred = [y_train[i] for i in k_neighbors]\n",
    "    return Counter(y_pred).most_common(1)[0][0] # Return the most common label (majority voting)\n",
    "\n",
    "def predict_knn_class(x_train, y_train, k, z): # KNN for classification\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(len(x_train)): # We need to Calculate the Euclidean distance from the test point (z) to all points in the training set\n",
    "        distance = EuclideanDistance(z, x_train[i])\n",
    "        distances.append((i, distance))  # Storing the index and corresponding distance\n",
    "\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    k_neighbors = [distances[i][0] for i in range(k)]  # taking indices of the k nearest neighbors\n",
    "    \n",
    "    return y_prediction(k_neighbors, y_train)  #  majority voting prediction\n",
    "\n",
    "def grid_search_knn(X, y, k_values): # Function to perform grid search for finding optimal K value\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    best_k = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for k in k_values:  # Trying different values of K and checking\n",
    "        y_train_pred = []\n",
    "        for i in range(len(X_train)):\n",
    "            y_train_pred.append(predict_knn_class(X_train, y_train, k, X_train[i]))  # Get predictions for training set\n",
    "\n",
    "        accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "        y_test_pred = []  # Predict for the test set\n",
    "        for i in range(len(X_test)):\n",
    "            y_test_pred.append(predict_knn_class(X_train, y_train, k, X_test[i]))  # Getting test set predictions\n",
    "\n",
    "        accuracy_test = accuracy_score(y_test, y_test_pred) \n",
    "\n",
    "    \n",
    "        if accuracy_test > best_accuracy:\n",
    "            best_accuracy = accuracy_test\n",
    "            best_k = k\n",
    "\n",
    "        print(f\"K={k}, Train Accuracy: {accuracy_train:.4f}, Test Accuracy: {accuracy_test:.4f}\")\n",
    "\n",
    "    print(f\"Optimal K: {best_k}\")\n",
    "    print(f\"Best Test Accuracy with K={best_k}: {best_accuracy:.4f}\")\n",
    "\n",
    "    return best_k, best_accuracy\n",
    "\n",
    "def main_knn_example():\n",
    "    \n",
    "    # Creating dataset (500 rsamples, 5 features)  ## There were no target columns in the dodgr loop dataset\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(500, 5)  # \n",
    "    y = np.random.randint(0, 2, 500)  # 0 or 1\n",
    "\n",
    "    k_values = [1, 3, 5, 7, 9]  # Perform grid search over different K values\n",
    "    grid_search_knn(X, y, k_values)\n",
    "\n",
    "main_knn_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "69b41e98-ab13-4e59-adcf-5ef8a7412e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6    7     8    9    ...   279  280  \\\n",
       "0  1.0   7.0   3.0   6.0  11.0   8.0   6.0  6.0  10.0  4.0  ...  12.0  5.0   \n",
       "1  1.0   9.0  10.0   5.0   7.0  10.0   9.0  5.0   6.0  8.0  ...   8.0  5.0   \n",
       "2  1.0  12.0  18.0  11.0  11.0  19.0  17.0  4.0   6.0  8.0  ...  10.0  9.0   \n",
       "\n",
       "    281  282  283   284  285  286   287  288  \n",
       "0   9.0  4.0  4.0   6.0  9.0  5.0  16.0  8.0  \n",
       "1   4.0  8.0  6.0  11.0  5.0  8.0   9.0  6.0  \n",
       "2  11.0  8.0  4.0   7.0  3.0  6.0   3.0  6.0  \n",
       "\n",
       "[3 rows x 289 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9c312e5b-3b63-4ed6-9792-9844cf57fdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1       9.0\n",
       "2      10.0\n",
       "3       5.0\n",
       "4       7.0\n",
       "       ... \n",
       "284    11.0\n",
       "285     5.0\n",
       "286     8.0\n",
       "287     9.0\n",
       "288     6.0\n",
       "Name: 1, Length: 289, dtype: float64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## df1.iloc[1] #better for numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985960aa-30c6-4360-ba4d-dbd2389dc500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa2ace-9efa-42e9-a47c-b3c05f7d4d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b77924-955a-495f-bb2c-a902ffd2579d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33067ea-335a-4596-bc4e-16bb7807f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Voronoi tesselation (Decision boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c88a2b6-a13b-495e-8f70-c1cd30db1274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Generate random 2D points (training data)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(10, 2) * 10  # 10 points in 2D space\n",
    "y_train = np.random.choice(['A', 'B'], size=10)  # Two class labels\n",
    "\n",
    "# Train a 1-NN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Compute Voronoi diagram\n",
    "vor = Voronoi(X_train)\n",
    "\n",
    "# Plot Voronoi tessellation\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='black')\n",
    "\n",
    "# Scatter plot of training points\n",
    "for label in np.unique(y_train):\n",
    "    mask = y_train == label\n",
    "    ax.scatter(X_train[mask, 0], X_train[mask, 1], label=f\"Class {label}\", edgecolor='k')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Voronoi Tesselation for 1-NN Decision Boundaries\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5b2e6-a186-40af-9b5e-20eac8cf151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Implementing Kfold cross validation to find the optimal value of k using error minimization\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=500, n_features=5, n_classes=2, n_informative=3, n_redundant=2, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implement K-Fold Cross-Validation to find the optimal K for KNN\n",
    "k_values = range(1, 21)  # Checking K values from 1 to 20\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-Fold Cross Validation\n",
    "\n",
    "error_rates = []\n",
    "\n",
    "for k in k_values:\n",
    "    fold_errors = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # Train KNN model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predict on validation fold\n",
    "        y_val_pred = knn.predict(X_val_fold)\n",
    "        \n",
    "        # Compute error rate (1 - accuracy)\n",
    "        fold_error = 1 - accuracy_score(y_val_fold, y_val_pred)\n",
    "        fold_errors.append(fold_error)\n",
    "    \n",
    "    # Store mean error across folds for this K value\n",
    "    error_rates.append(np.mean(fold_errors))\n",
    "\n",
    "# Find optimal K (with the lowest error rate)\n",
    "optimal_k = k_values[np.argmin(error_rates)]\n",
    "optimal_error = min(error_rates)\n",
    "\n",
    "# Plot error rates vs K values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, error_rates, marker='o', linestyle='dashed', color='b', label=\"Error Rate\")\n",
    "plt.axvline(optimal_k, color='r', linestyle='--', label=f\"Optimal K={optimal_k}\")\n",
    "plt.xlabel(\"Number of Neighbors (K)\")\n",
    "plt.ylabel(\"Mean Error Rate\")\n",
    "plt.title(\"K-Fold Cross-Validation for Optimal K in KNN\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train final model with optimal K\n",
    "final_knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "final_knn.fit(X_train, y_train)\n",
    "y_test_pred = final_knn.predict(X_test)\n",
    "\n",
    "# Compute final accuracy on test data\n",
    "final_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Display results\n",
    "final_knn_results_df = pd.DataFrame({\n",
    "    \"Parameter\": [\"Optimal K\", \"Final Test Accuracy\"],\n",
    "    \"Value\": [optimal_k, final_accuracy]\n",
    "})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Optimal KNN Results\", dataframe=final_knn_results_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####another solu\n",
    "\n",
    "def EuclideanDistance(a,b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n",
    "    \n",
    "def y_prediction(x,y,k):\n",
    "    y_pred=[]\n",
    "    for k,v in x:\n",
    "        y_pred.append(y[k,0])\n",
    "    return Counter(y_pred).most_common(1)[0][0]\n",
    "\n",
    "def y_prediction(x,y,k):\n",
    "    y_pred=[]\n",
    "    for k,v in x:\n",
    "        y_pred.append(y[k,0])\n",
    "    return Counter(y_pred).most_common(1)[0][0]\n",
    "\n",
    "def predict_knn_reg(x,y,k,z):\n",
    "    d=dict()\n",
    "    for i in range(0,len(x)):\n",
    "        d[i]=EuclideanDistance(z,x.iloc[i])\n",
    "    d_new=sorted(d.items(),key=lambda kv:kv[1])\n",
    "    c=d_new[:k]\n",
    "    y_pred=y_prediction(c,y,k)\n",
    "    return y_pred\n",
    "\n",
    "def data_k_divide(data,k):\n",
    "    k_size=Math.floor(len(data)/k)\n",
    "    k_data=[] \n",
    "    c=0\n",
    "    for i in range (0,k):\n",
    "        data_set=pd.DataFrame(data.head(0))\n",
    "        for j in range(i*k_size,(i*k_size)+k_size):\n",
    "            data_set=data_set.append(data.iloc[j])\n",
    "            c=c+1\n",
    "        k_data.append(data_set)\n",
    "        \n",
    "    #adding datas which are remaining at the end of k division\n",
    "    for j in range(c,len(data)):\n",
    "        k_data[k-1]=k_data[k-1].append(data.iloc[j])\n",
    "    return k_data\n",
    "\n",
    "def k_data_train_test(x,y,k):\n",
    "    k_folded_data=[]\n",
    "    for i in range(0,k):\n",
    "        x_test=x[i]\n",
    "        y_test=y[i]\n",
    "        x_train=pd.DataFrame()\n",
    "        y_train=pd.DataFrame()\n",
    "        for j in range(0,k):\n",
    "            if i!=j:\n",
    "                x_train=x_train.append(x[j])\n",
    "                y_train=y_train.append(y[j])\n",
    "        final_data=dict([('x',x_train),('y',y_train),('xt',x_test),('yt',y_test)])\n",
    "        k_folded_data.append(final_data)\n",
    "    return k_folded_data\n",
    "\n",
    "def kfold(x_train,y_train,k):\n",
    "    x_train_k=data_k_divide(x_train,k)\n",
    "    y_train_k=data_k_divide(y_train,k)\n",
    "    data=k_data_train_test(x_train_k,y_train_k,k)\n",
    "    accuracy=[]\n",
    "    for i in range(len(data)):\n",
    "        y_pred=[]\n",
    "        for j in range(0,len(data[i]['xt'])):\n",
    "            y_pred.append(predict_knn_reg(data[i]['x'],data[i]['y'].values,k,data[i]['xt'].iloc[j].values))\n",
    "        accuracy.append(classification_accuracy(data[i]['yt'],y_pred))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def classification_accuracy(y,y_pred):\n",
    "    y=pd.DataFrame(y).to_numpy()\n",
    "    y_pred=np.asarray(y_pred)\n",
    "    acc=np.mean(y == y_pred)\n",
    "    return acc\n",
    "\n",
    "\n",
    "ypred_v=[]\n",
    "k=5\n",
    "for j in range(0,len(x_test_rwine)):\n",
    "    ypred_v.append(predict_knn_reg(x_train_rwine,y_train_rwine.values,k,x_test_rwine.iloc[j].values))\n",
    "\n",
    "\n",
    "best_accuracy=0\n",
    "best_k=0\n",
    "accuracyall=[]\n",
    "for i in range (2,15): #using k values between 2 to 15\n",
    "    kr=kfold(x_train_rwine,y_train_rwine,i)\n",
    "    if(len(accuracyall)==0 or np.average(kr)>best_accuracy):\n",
    "        best_k=i\n",
    "        best_accuracy=np.average(kr)\n",
    "    accuracyall.append(np.average(kr))\n",
    "    print('for value of k=',i,' ','best k=',best_k,'best_accuracy=',best_accuracy)\n",
    "print('best average accuracy=',best_accuracy)\n",
    "print('best k=',best_k) \n",
    "\n",
    "fig,axs=plt.subplots(1,1,figsize=(20,10))\n",
    "axs.plot(np.arange(len(accuracyall)),accuracyall)\n",
    "axs.plot(np.arange(len(accuracyall)),accuracyall,'o')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f5ba3-d606-4f28-bb54-f3ddeba60746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653d865-66b8-403f-8e2f-b3045217b019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f46b922-4016-43c3-ac42-95ee73aa461e",
   "metadata": {},
   "source": [
    "### 2. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4c7672-6a1e-4b37-8ec3-f611d80df298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4\n",
       "Iris-setosa        50\n",
       "Iris-versicolor    50\n",
       "Iris-virginica     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('iris\\iris.data',header=None)\n",
    "df3[4].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fe2d0662-51de-4d37-b6e8-8cfe429f7aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4_Iris-versicolor</th>\n",
       "      <th>4_Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3  4_Iris-versicolor  4_Iris-virginica\n",
       "0  5.1  3.5  1.4  0.2              False             False\n",
       "1  4.9  3.0  1.4  0.2              False             False\n",
       "2  4.7  3.2  1.3  0.2              False             False\n",
       "3  4.6  3.1  1.5  0.2              False             False\n",
       "4  5.0  3.6  1.4  0.2              False             False"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df4 = pd.get_dummies(df3, columns=[4], drop_first=True)\n",
    "# df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dedbd34-6895-4102-b383-9b85ce80377b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  5.1  3.5  1.4  0.2  0.0\n",
       "1  4.9  3.0  1.4  0.2  0.0\n",
       "2  4.7  3.2  1.3  0.2  0.0\n",
       "3  4.6  3.1  1.5  0.2  0.0\n",
       "4  5.0  3.6  1.4  0.2  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[4] = df3[4].map({'Iris-setosa':'0', 'Iris-versicolor':'1', 'Iris-virginica':'2'})\n",
    "df3[4] = df3[4].astype(float)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b476e7-6b75-4d1e-9f11-c73220eebade",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df3.drop(columns=4)\n",
    "y = df3[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc5460-02c6-4e02-b267-635ee13520ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d60f84-5346-45f4-b0f9-ecbdf8dcfba4",
   "metadata": {},
   "source": [
    "#### a) Class for Tree Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40735535-c219-4854-854b-c46eac4eeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature  \n",
    "        self.threshold = threshold  \n",
    "        self.left = left  \n",
    "        self.right = right  \n",
    "        self.value = value  # Predicted value for leaf nodes\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b556a1f-067b-48ec-b119-83402ed017e2",
   "metadata": {},
   "source": [
    "#### b) Functions to build the Tree using RSS as the criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c599368-a5f4-4b1f-b6d1-3f5247a30599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss(y):\n",
    "    if len(y) == 0:  # Handle empty data\n",
    "        return 0\n",
    "    mean_y = np.mean(y)\n",
    "    return np.sum((y - mean_y) ** 2)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_feats = X.shape[1]\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    # def predict(self, X):\n",
    "    #     return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "        \n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or n_samples < self.min_samples_split):\n",
    "            leaf_value = np.mean(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
    "\n",
    "        # Greedy search\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "\n",
    "        # Growing the children\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            for threshold in thresholds:\n",
    "                gain = self._calculate_rss(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "\n",
    "    def _calculate_rss(self, y, X_column, split_thresh):\n",
    "        \n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "        \n",
    "        # If no split (empty subset), return infinity to avoid it\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return float(\"inf\")\n",
    "        \n",
    "        rss_left = rss(y[left_idxs])\n",
    "        rss_right = rss(y[right_idxs])\n",
    "\n",
    "        return rss_left + rss_right\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "    \n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            print(f\"Warning: One of the splits is empty (threshold: {split_thresh}).\")\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "\n",
    "    # def _traverse_tree(self, x, node):\n",
    "    #     if node.is_leaf_node():\n",
    "    #         if node.value is None:\n",
    "    #             print(\"error: leaf node value is None\")\n",
    "    #             raise ValueError(\"leaf node has no value.\")\n",
    "    #         return node.value\n",
    "    \n",
    "    #     if node.feature is None or node.threshold is None:\n",
    "    #         print(\"Error: Node feature or threshold is None\")\n",
    "    #         raise ValueError(\"Node feature or threshold is None during traversal.\")\n",
    "    \n",
    "    #     print(f\"Traversing node: feature {node.feature}, threshold {node.threshold}, value {node.value}\")\n",
    "    #     if x[node.feature] <= node.threshold:\n",
    "    #         return self._traverse_tree(x, node.left)\n",
    "    #     return self._traverse_tree(x, node.right)\n",
    "\n",
    " \n",
    "\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# x = np.array(x)\n",
    "# y = np.array(y).flatten()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "# clf = DecisionTree(max_depth=10)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "#  # Make predictions on the test set\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# mse = clf.mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea57025-3427-4537-8875-fdd5620ba5f9",
   "metadata": {},
   "source": [
    "#### c) Transversing the tree and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6883aa05-057c-45a4-b076-86459c3d0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _traverse_tree(self, x, node):\n",
    "    if node.is_leaf_node():\n",
    "        return node.value\n",
    "\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return self._traverse_tree(x, node.left)\n",
    "    return self._traverse_tree(x, node.right)\n",
    "\n",
    "\n",
    "def predict(self, X):\n",
    "    return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load the Iris dataset\n",
    "#     iris = load_iris()\n",
    "#     X, y = iris.data, iris.target\n",
    "#     y = y.astype(float)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Train the decision tree regressor\n",
    "#     tree = DecisionTree(min_samples_split=3, max_depth=10)\n",
    "#     tree.fit(X_train, y_train)\n",
    "\n",
    "#     # Make predictions and evaluate\n",
    "#     y_pred = tree.predict(X_test)\n",
    "#     mse = tree.mean_squared_error(y_test, y_pred)\n",
    "#     print(f\"Mean Squared Error on test set: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4d0e7-85c6-4548-b580-342ca653d92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee6174-feaf-4f61-be66-6fdf9aaddc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8132ca-5a40-403b-8c00-55d8ef6f8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "class CustomDecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # should set a n_features_in_ attribute at fit time to indicate the number of features that the estimator expects for \n",
    "        #subsequent calls to predict or transform\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        \n",
    "        data = np.c_[X, y]\n",
    "        \n",
    "        self.tree_ = Decision_Tree(name=\"root\", level=0, max_depth=self.max_depth)\n",
    "        self.tree_.expand_decision_tree(data)\n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check if fit has been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        if X.shape[1] != self.n_features_in_:\n",
    "            raise ValueError('Shape of input is different from what was seen in `fit`')\n",
    "        \n",
    "        return self.tree_(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec853a-df08-45f0-8414-01320157ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "check_estimator(CustomDecisionTreeClassifier())\n",
    "\n",
    "class Decision_Tree():\n",
    "    def __init__(self, name=\"root\", level=0, max_depth=None, log=False):\n",
    "        self.log = log\n",
    "        self.name = name\n",
    "        self.is_leaf = False\n",
    "        self.level = level\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def __call__ (self , X, log=False):\n",
    "        if self.is_leaf:\n",
    "            return np.full(len(X), self.value)\n",
    "        else:\n",
    "\n",
    "            split_rule = X[:, self.split_column].astype(float) < self.split_value #generate the array of boolean values with shape(X.shape[0], 1)\n",
    "            self_left_result = self.left(X[split_rule])\n",
    "            \n",
    "            if len(self_left_result)>0 and isinstance(self_left_result[0], str):\n",
    "                #y_pred = np.full(len(X), \"\")\n",
    "                y_pred = np.empty((len(X)), dtype=np.dtype('U100'))\n",
    "            else:\n",
    "                y_pred = np.full(len(X), np.nan)\n",
    "            \n",
    "            self_right_result = self.right(X[~split_rule])\n",
    "            \n",
    "            y_pred[split_rule] = self_left_result\n",
    "            y_pred[~split_rule] = self_right_result\n",
    "            \n",
    "            return y_pred\n",
    "        \n",
    "    def expand_decision_tree(self, data):\n",
    "        if(self.log):\n",
    "            print(self.name + \" - Going to start growing\")\n",
    "            print(self.name + \" - Number samples: \" + str(data.shape[0]))\n",
    "        \n",
    "        if(self.stopping_criterion(data)):\n",
    "            #It's a leaf, all remaining data falls under this leaf\n",
    "            self.is_leaf = True\n",
    "            self.value = self.classify(data)\n",
    "            if(self.log):\n",
    "                print(self.name + \" - Value:\", self.value)\n",
    "            return self\n",
    "        \n",
    "        #It's not a leaf, the tree keeps growing\n",
    "        \n",
    "        #TODO\n",
    "        candidate_splits = self.get_candidate_splits(data)\n",
    "        if(self.log):\n",
    "            print(self.name + \" - candidate_splits:\", candidate_splits)\n",
    "        self.split_column, self.split_value = self.get_best_split(data, candidate_splits)\n",
    "        self.value = \"< \" + str(self.split_value)\n",
    "        if(self.log):\n",
    "            print(self.name + \" - self.split_column:\", self.split_column)\n",
    "            print(self.name + \" - self.split_value:\", self.split_value)\n",
    "        data_below, data_above = self.split_data(data, self.split_column, self.split_value)\n",
    "        if(self.log):\n",
    "            print(self.name + \" - data_below:\\n\", data_below)\n",
    "            print(self.name + \" - data_above:\\n\", data_above)\n",
    "        self.left = Decision_Tree(name=self.name + \" - \" + str(self.level+1) + \"-left\", level=self.level+1, max_depth=self.max_depth).expand_decision_tree(data_below)\n",
    "        self.right = Decision_Tree(name=self.name + \" - \" + str(self.level+1) + \"-right\", level=self.level+1, max_depth=self.max_depth).expand_decision_tree(data_above)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def learn_from_data(self, data):\n",
    "        self.expand_decision_tree(data)\n",
    "            \n",
    "    def stopping_criterion(self, data):\n",
    "        if self.max_depth != None and self.level + 1 == self.max_depth:\n",
    "            if(self.log):\n",
    "                print(self.name + \" - Has reached max levels:\", self.max_depth)\n",
    "            return True\n",
    "        \n",
    "        #If there is only one left node or all remaining nodes belong to one class, this is a leaf\n",
    "        class_column = data[:, -1]\n",
    "        if(self.log):\n",
    "            print(self.name + \" - Has only one element:\", data.shape[0] == 1)\n",
    "            print(self.name + \" - Only one class:\", len(np.unique(class_column)) == 1)\n",
    "        \n",
    "        if data.shape[0] == 1 or len(np.unique(class_column)) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def classify(self, data):\n",
    "        class_column = data[:, -1]\n",
    "        unique_classes, unique_classes_counts = np.unique(class_column, return_counts=True)\n",
    "        \n",
    "        index_majority_class =np.argmax(unique_classes_counts)\n",
    "        class_value = unique_classes[index_majority_class]\n",
    "        return class_value\n",
    "    \n",
    "    def get_candidate_splits(self, data):\n",
    "        candidate_splits = {}\n",
    "        features = data.shape[1] - 1 # remove last class column\n",
    "        if(self.log):\n",
    "            print(self.name + \" - features:\", features)\n",
    "        for feature_idx in np.arange(features):\n",
    "            candidate_splits[feature_idx] = []\n",
    "            feature_unique_values = np.unique(data[:, feature_idx])\n",
    "            if(self.log):\n",
    "                print(self.name + \" - Feature [\", feature_idx, \"] feature_unique_values:\", feature_unique_values)\n",
    "            for idx in np.arange(1, len(feature_unique_values)):\n",
    "                prev_val = feature_unique_values[idx - 1]\n",
    "                curr_val = feature_unique_values[idx]\n",
    "                candidate_split = (float(curr_val) + float(prev_val))/2\n",
    "                \n",
    "                candidate_splits[feature_idx].append(candidate_split)\n",
    "                \n",
    "        return candidate_splits\n",
    "    \n",
    "    def split_data(self, data, split_column, split_value):\n",
    "        split_column_values = data[:, split_column].astype(float)\n",
    "\n",
    "        data_below = data[split_column_values < split_value]\n",
    "        data_above = data[split_column_values >=  split_value]\n",
    "\n",
    "        return data_below, data_above\n",
    "    \n",
    "    def get_best_split(self, data, candidate_splits):\n",
    "        best_information_gain = 0\n",
    "        # select the split rule value according to the quality criterion\n",
    "        for feature_idx in candidate_splits:\n",
    "            for split in candidate_splits[feature_idx]:\n",
    "                data_below, data_above = self.split_data(data, feature_idx, split)\n",
    "                if(self.log):\n",
    "                    print(self.name + \" - Evaluate split:\", split)\n",
    "                curr_information_gain = self.calculate_information_gain(data_below, data_above)\n",
    "                if(self.log):\n",
    "                    print(self.name + \" - Feature [\", feature_idx, \"] split [\", split, \"] curr_information_gain:\", curr_information_gain)\n",
    "                if curr_information_gain > best_information_gain:\n",
    "                    best_information_gain = curr_information_gain\n",
    "                    best_split_column = feature_idx\n",
    "                    best_split_value = split\n",
    "                \n",
    "        return best_split_column, best_split_value\n",
    "        #return 0, 6.1\n",
    "    \n",
    "    def calculate_information_gain(self, data_below, data_above):\n",
    "        if(self.log):\n",
    "            print(self.name + \" - data_below.shape:\", data_below.shape)\n",
    "            print(self.name + \" - data_above.shape:\", data_above.shape)\n",
    "        sum_of_matrix = len(data_below) + len(data_above)\n",
    "        cj_left = len(data_below)  #number of samples of row left\n",
    "        cj_right = len(data_above) #number of samples of row right\n",
    "        \n",
    "        _, cjk_left = np.unique(data_below[:,-1], return_counts=True) #vector with number of samples per each class of row left\n",
    "        _, cjk_right = np.unique(data_above[:,-1], return_counts=True) #vector with number of samples per each class of row right\n",
    "        \n",
    "        class_column = np.concatenate((data_below,data_above),axis=0)[:,-1]\n",
    "        _, ck = np.unique(class_column, return_counts=True) #vector with number of samples per each class\n",
    "        \n",
    "        if(self.log):\n",
    "            print(self.name + \" - sum_of_matrix:\", sum_of_matrix)\n",
    "            print(self.name + \" - cj_left:\", cj_left)\n",
    "            print(self.name + \" - cj_right:\", cj_right)\n",
    "            print(self.name + \" - cjk_left:\", cjk_left)\n",
    "            print(self.name + \" - cjk_right:\", cjk_right)\n",
    "            print(self.name + \" - ck:\", ck)\n",
    "        \n",
    "        information_gain =  -sum(ck/sum_of_matrix * np.log2(ck/sum_of_matrix)) \\\n",
    "                            + cj_left/sum_of_matrix  * sum(cjk_left/cj_left * np.log2(cjk_left/cj_left)) \\\n",
    "                            + cj_right/sum_of_matrix * sum(cjk_right/cj_right * np.log2(cjk_right/cj_right))\n",
    "                                                           \n",
    "        if(self.log):\n",
    "            print(self.name + \" - information_gain\", information_gain)\n",
    "        return information_gain\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ff26c-c6bb-4390-8c7b-9a14cad8aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    #normalization\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # apply normalization to cont cols of subset\n",
    "    X = sc.fit_transform(X)\n",
    "    #print(sc.mean_)\n",
    "    \n",
    "    return X\n",
    "\n",
    "#Read Iris data\n",
    "iris_dataset= datasets.load_iris()\n",
    "X = iris_dataset.data\n",
    "y = iris_dataset.target\n",
    "X = normalize(X)\n",
    "print(\"Samples:\", X.shape)\n",
    "print(\"Target:\", y.shape)\n",
    "print(\"Target Classes:\")\n",
    "print(list(iris_dataset.target_names))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2023)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "custom_cfl = CustomDecisionTreeClassifier()\n",
    "custom_cfl.fit(X_train, y_train)\n",
    "y_pred = custom_cfl.predict(X_test)\n",
    "print(\"Custom DecisionTreeClassifier\")\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_pred))\n",
    "print(\"Number of correctly classified samples:\", accuracy_score(y_test, y_pred, normalize=False), \"/\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bafa20-ffec-45d2-88ae-06c53bd09f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=2023)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Sklearn DecisionTreeClassifier\")\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_pred))\n",
    "print(\"Number of correctly classified samples:\", accuracy_score(y_test, y_pred, normalize=False), \"/\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad9ab17-29b7-4f97-8396-e09bdde2bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_dataset= datasets.load_breast_cancer()\n",
    "X = bc_dataset.data\n",
    "y = bc_dataset.target\n",
    "X = normalize(X)\n",
    "print(\"Samples:\", X.shape)\n",
    "print(\"Target:\", y.shape)\n",
    "print(\"Target Classes:\")\n",
    "print(list(bc_dataset.target_names))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2023)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e92b3-6d0e-4929-b3b7-14de0f1df738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c802fee-ee29-4513-a1f2-8b8557d994bb",
   "metadata": {},
   "source": [
    "**Decision Trees**\n",
    "\n",
    "In this part, you need to implement a decision tree for classification. \n",
    "\n",
    "- Implement an object class **\"Decision_Tree\"** with learn and predict methods. The class should work with multiple **Quality-criterion**. (Accuracy, Information Gain, Misclassification Rate (MCR)) \n",
    "- Implement appropriate stopping criterion i.e. max depth, gain is too small, minimum number of samples for splitting. You can have one or more stopping criterias.\n",
    "- Download and read the Nursery dataset. Link: https://archive.ics.uci.edu/ml/datasets/Nursery\n",
    "- Once the data is loaded, split the data into 70-20-10 split for train/validation/test. *(You can use sklearn for splitting the dataset)*\n",
    "- Train your **\"Decision_Tree\"** with different hyperparameters\n",
    "    - Perform either grid or random search. *(You can use sklearn for hyperparameter search)*\n",
    "    - Hyperparameters can include max-depth, minimunm gain for splitting, minimum number of samples for splitting. Quality-criterion must be one of the hyperparameter. \n",
    "    - Compare the results on validation data. \n",
    "    - Report the test results for the best model. \n",
    "    - Print the best tree using a breath first tree traversal (only till depth of 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af07afe-036f-4112-870d-6ca4854747e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data\n",
    "df = pd.read_csv(\"nursery_data.txt\")\n",
    "#splitting into X and y columns and encoding the target labels\n",
    "X, y = df.iloc[:,:-1],df.iloc[:,-1]\n",
    "target_encoder = LabelEncoder()\n",
    "y_transformed = target_encoder.fit_transform(y)\n",
    "\n",
    "#splitting based on train, valid, test ratios\n",
    "train_ratio = 0.70 \n",
    "validation_ratio = 0.20\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 70% of the entire data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=1 - train_ratio, random_state=3116)\n",
    "\n",
    "# test is now 15% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=3116) \n",
    "\n",
    "\n",
    "#make dataframes of all the training, validation and test sets\n",
    "y_train = pd.DataFrame(y_train,index=X_train.index)\n",
    "y_test = pd.DataFrame(y_test,index=X_test.index)\n",
    "y_valid = pd.DataFrame(y_valid,index=X_valid.index)\n",
    "\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Class which represents a simple node in a decision tree.\n",
    "    \"\"\"\n",
    "    def __init__ (self, probs=None, feature=None, condition=None, left_subtree=None, right_subtree=None, leaf_val=None):\n",
    "        # Probability of an instance belonging to different classes\n",
    "        self.probs = probs\n",
    "        # The feature which the node checks\n",
    "        self.feature = feature\n",
    "        # The condition or threshold\n",
    "        self.condition = condition\n",
    "        # Left subtree of the node\n",
    "        self.left_subtree = left_subtree\n",
    "        # Right subtree of the node\n",
    "        self.right_subtree = right_subtree\n",
    "        # Leaf value that the node has\n",
    "        self.leaf_val = leaf_val\n",
    "\n",
    "class DecisionTreeClf:\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier Class which implements the Decision tree based on MCR as a quality criterion and Information Gain.\n",
    "    \"\"\"\n",
    "    def __init__ (self, criterion, max_depth, min_samples_per_leaf, classes_name, tolerance=0.01):\n",
    "        # Maximum depth to which the tree can grow\n",
    "        self.max_depth = max_depth\n",
    "        # Minimum number of samples in each node\n",
    "        self.min_samples_per_leaf = min_samples_per_leaf\n",
    "        # Root node\n",
    "        self.root = None\n",
    "        # Criterion, can be MCR or Entropy\n",
    "        self.criterion = criterion\n",
    "        # Names of classes which the data represents\n",
    "        self.classes_name = classes_name\n",
    "        # List to store gains at each decision step, useful for visualizations\n",
    "        self.best_gains = []\n",
    "        # Stopping criteria when gain is too small or reduction in cost is small\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # It is important to know the types of columns\n",
    "        self.cat_types = self.cols_cat_list(X)\n",
    "        # Number of classes in the data\n",
    "        self.num_cats = np.max(np.unique(y)) + 1\n",
    "        # Concatenating data and target as they are easier to deal with\n",
    "        data = pd.concat([X, y], axis=1)\n",
    "        # Growing the tree from the root node\n",
    "        self.root = self.grow_tree(data)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # List to store predictions\n",
    "        preds = []\n",
    "        # Iterating over one row at a time\n",
    "        for _, x in X.iterrows():\n",
    "            # Storing the predictions for each observation\n",
    "            preds.append(self.forward_traverse(x, self.root)[0])\n",
    "        return preds\n",
    "\n",
    "    def misclassification_rate(self, y):\n",
    "        # Initialize the empty vector which represents the probability of each class\n",
    "        pred = np.zeros(self.num_cats)\n",
    "        # Count the number of unique classes\n",
    "        val, counts = np.unique(y, return_counts=True)\n",
    "        # Set the count as the initial prediction (in the next step, this will be converted to probability)\n",
    "        pred[val] = counts\n",
    "        # Convert the count into probability and return the negative MCR\n",
    "        probs = pred / np.sum(pred)\n",
    "        return -(1 - np.max(probs))\n",
    "\n",
    "    def entropy(self, y):\n",
    "        # To calculate the information gain, the entropy function will be useful\n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        # Iterate over each class and calculate its entropy\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, parent, left_y, right_y):\n",
    "        # Weight of left and right nodes\n",
    "        weight_l = len(left_y) / len(parent)\n",
    "        weight_r = len(right_y) / len(parent)\n",
    "        # Quality criterion as MCR and Information gain are handled differently\n",
    "        if self.criterion == \"MCR\":\n",
    "            return weight_l * self.misclassification_rate(left_y) + weight_r * self.misclassification_rate(right_y)\n",
    "        elif self.criterion == \"Entropy\":\n",
    "            return self.entropy(parent) - (weight_l * self.entropy(left_y) + weight_r * self.entropy(right_y))\n",
    "\n",
    "    def split_data(self, data, feature, condition):\n",
    "        # Get the datatype of the presented feature\n",
    "        data_type = data[feature].dtype\n",
    "        # Numeric and categoric columns are handled differently\n",
    "        if data_type == 'O':\n",
    "            left_data = data[data[feature] == condition]\n",
    "            right_data = data[data[feature] != condition]\n",
    "        else:\n",
    "            left_data = data[data[feature] < condition]\n",
    "            right_data = data[data[feature] >= condition]\n",
    "        return left_data, right_data\n",
    "\n",
    "    def get_leaf_value(self, y):\n",
    "        # Initialize the empty probability vector\n",
    "        pred = np.zeros(self.num_cats)\n",
    "        # Count unique ys\n",
    "        val, counts = np.unique(y, return_counts=True)\n",
    "        # Set the count to the probability vector\n",
    "        pred[val] = counts\n",
    "        # Convert counts into probability\n",
    "        probs = pred / np.sum(pred)\n",
    "        return probs\n",
    "\n",
    "    def cols_cat_list(self, data):\n",
    "        # This function helps in separating categoric columns from numeric ones\n",
    "        col_types = {}\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == 'O':\n",
    "                col_types[col] = 'cat'\n",
    "            else:\n",
    "                col_types[col] = 'num'\n",
    "        return col_types\n",
    "\n",
    "    def best_split(self, data):\n",
    "        # Dictionary to store information about the best split\n",
    "        best_split = {}\n",
    "        # Since we want to maximize the gain, start with the smallest number\n",
    "        best_gain = -np.inf\n",
    "        # Start iterating over each column, one by one till the target\n",
    "        for feat in data.columns[:-1]:\n",
    "            # Numeric and categorical column will be handled differently\n",
    "            data_type = data[feat].dtype\n",
    "            # For categorical features\n",
    "            if data_type == 'O':\n",
    "                v = np.unique(data[feat])\n",
    "                # Iterate over each unique value\n",
    "                for cond in v:\n",
    "                    # Splitting the data based on the current value\n",
    "                    left_data, right_data = self.split_data(data, feat, cond)\n",
    "                    # Avoid unnecessary splits\n",
    "                    if len(left_data) > 0 and len(right_data) > 0:\n",
    "                        # Get the labels of each dataset\n",
    "                        data_y, left_y, right_y = data.iloc[:, -1], left_data.iloc[:, -1], right_data.iloc[:, -1]\n",
    "                        # Calculate the quality criterion/information gain of the current split\n",
    "                        gain = self.information_gain(data_y, left_y, right_y)\n",
    "                        # Store the information of the best split\n",
    "                        if gain > best_gain:\n",
    "                            best_gain = gain\n",
    "                            best_split['feature'] = feat\n",
    "                            best_split['condition'] = cond\n",
    "                            best_split['gain'] = gain\n",
    "                            best_split['left_data'] = left_data\n",
    "                            best_split['right_data'] = right_data\n",
    "            # For numeric features\n",
    "            else:\n",
    "                v = sorted(list(set(data[feat])))\n",
    "                # Arrange all the unique numeric column into ascending order\n",
    "                means = [(v[i] + v[i + 1]) / 2 for i in range(len(v) - 1)]\n",
    "                # Check on each threshold of the mean of entries\n",
    "                for mean in means:\n",
    "                    # Splitting the data\n",
    "                    left_data, right_data = self.split_data(data, feat, mean)\n",
    "                    # Avoid unnecessary splits\n",
    "                    if len(left_data) > 0 and len(right_data) > 0:\n",
    "                        # Get the labels of each dataset\n",
    "                        data_y, left_y, right_y = data.iloc[:, -1], left_data.iloc[:, -1], right_data.iloc[:, -1]\n",
    "                        # Calculate the quality criterion/information gain of the current split\n",
    "                        gain = self.information_gain(data_y, left_y, right_y)\n",
    "                        # Store the information of the best split\n",
    "                        if gain > best_gain:\n",
    "                            best_gain = gain\n",
    "                            best_split['feature'] = feat\n",
    "                            best_split['condition'] = mean\n",
    "                            best_split['gain'] = gain\n",
    "                            best_split['left_data'] = left_data\n",
    "                            best_split['right_data'] = right_data\n",
    "        return best_split\n",
    "\n",
    "    def grow_tree(self, data, curr_depth=0):\n",
    "        # Separate the labels\n",
    "        labels = data.iloc[:, -1]\n",
    "        # Count the unique labels in the target set\n",
    "        n_labels = len(np.unique(labels))\n",
    "        # Count the number of samples in the data\n",
    "        samples = data.shape[0]\n",
    "        # Get the empty vector to store probabilities\n",
    "        probs = np.zeros(self.num_cats)\n",
    "        p = labels.value_counts() / labels.value_counts().sum()\n",
    "        # Get the probability of each class\n",
    "        for i, v in p.items():\n",
    "            probs[i] = v\n",
    "        # Checking the stopping condition\n",
    "        if samples < self.min_samples_per_leaf or n_labels == 1 or curr_depth > self.max_depth:\n",
    "            return Node(probs, leaf_val=np.argmax(self.get_leaf_value(data.iloc[:, -1])))\n",
    "        # If stopping condition is not met, then we will split the data based on the best split\n",
    "        best_split = self.best_split(data)\n",
    "        feat = best_split['feature']\n",
    "        condition = best_split['condition']\n",
    "        gain = best_split['gain']\n",
    "        left_data = best_split['left_data']\n",
    "        right_data = best_split['right_data']\n",
    "        # Check for early stopping based on the tolerance\n",
    "        if (len(self.best_gains) > 1) and np.abs(gain - self.best_gains[-1]) < self.tolerance:\n",
    "            return Node(probs, leaf_val=np.argmax(self.get_leaf_value(data.iloc[:, -1])))\n",
    "        else:\n",
    "            self.best_gains.append(gain)\n",
    "        # Adds left and right nodes recursively\n",
    "        left_subtree = self.grow_tree(left_data, curr_depth + 1)\n",
    "        right_subtree = self.grow_tree(right_data, curr_depth + 1)\n",
    "        return Node(probs, feat, condition, left_subtree, right_subtree)\n",
    "\n",
    "    def forward_traverse(self, data, node):\n",
    "        # Check if the current node is the leaf node\n",
    "        if node.leaf_val is not None:\n",
    "            return node.leaf_val, node.probs\n",
    "        # If it's a decision node\n",
    "        feat = node.feature\n",
    "        cond = node.condition\n",
    "        left_subtree = node.left_subtree\n",
    "        right_subtree = node.right_subtree\n",
    "        # Check the datatype of the feature\n",
    "        data_type = self.cat_types[feat]\n",
    "        # For categoric feature\n",
    "        if data_type == 'cat':\n",
    "            if data[feat] == cond:\n",
    "                return self.forward_traverse(data, left_subtree)\n",
    "            return self.forward_traverse(data, right_subtree)\n",
    "        # For numeric feature\n",
    "        else:\n",
    "            if data[feat] < cond:\n",
    "                return self.forward_traverse(data, left_subtree)\n",
    "            return self.forward_traverse(data, right_subtree)\n",
    "\n",
    "    def plot_histogram(self, node=None):\n",
    "        # This function plots the histogram at each decision split\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.leaf_val is not None:\n",
    "            plt.figure()\n",
    "            plt.xlabel(\"Target Class\")\n",
    "            plt.ylabel(\"Probability\")\n",
    "            plt.title(f\"Histogram of Probabilities at Leaf Node\")\n",
    "            plt.bar(self.classes_name, node.probs)\n",
    "            plt.show()\n",
    "        else:\n",
    "            feat = node.feature\n",
    "            cond = node.condition\n",
    "            plt.figure()\n",
    "            plt.xlabel(\"Target Class\")\n",
    "            plt.ylabel(\"Probability\")\n",
    "            plt.title(f\"Histogram of Probabilities at Decision {feat} == {cond}\")\n",
    "            plt.bar(self.classes_name, node.probs)\n",
    "            plt.show()\n",
    "            self.plot_histogram(node.left_subtree)\n",
    "            self.plot_histogram(node.right_subtree)\n",
    "\n",
    "    def print_tree(self, node=None, indent=1, max_depth=4):\n",
    "        # To print the tree\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.leaf_val is not None or indent > max_depth:\n",
    "            print(f\"Leaf Node Value: {self.classes_name[np.argmax(node.leaf_val)]}\")\n",
    "        else:\n",
    "            feat = node.feature\n",
    "            cond = node.condition\n",
    "            print(f\"Decision: {node.feature} == {node.condition}\")\n",
    "            print(\"|\" * indent + \"left :\", end=\"\")\n",
    "            self.print_tree(node.left_subtree, indent + 1, max_depth)\n",
    "            print(\"|\" * indent + \"right:\", end=\"\")\n",
    "            self.print_tree(node.right_subtree, indent + 1, max_depth)\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        # Calculate the loss of each sample\n",
    "        loss = []\n",
    "        for i in range(len(x)):\n",
    "            # Calculate the cross entropy loss of each sample\n",
    "            loss.append(np.log(self.forward_traverse(x.iloc[i, :], self.root)[1][y.iloc[i, :]][0] + 1e-12))\n",
    "        return -np.sum(loss) / len(loss)\n",
    "\n",
    "    def plot_information_gain(self):\n",
    "        # Plot the information gain\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Split Number\")\n",
    "        plt.ylabel(\"Information Gain\")\n",
    "        plt.title(f\"Plot of Information Gain at each Decision\")\n",
    "        plt.plot(np.arange(len(self.best_gains)), self.best_gains)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class TreeRandomSearchCV:\n",
    "    def __init__(self, config):\n",
    "        # Configuration dictionary for hyperparameters\n",
    "        self.config = config\n",
    "\n",
    "    def run(self, num_trials, X_data, y_data, X_valid, y_valid):\n",
    "        # Initialize best loss, best accuracy, best model, and best hyperparameters\n",
    "        best_loss = np.inf\n",
    "        best_acc = -np.inf\n",
    "        best_model = None\n",
    "        best_params = None\n",
    "\n",
    "        for _ in range(num_trials):\n",
    "            # Run the training function to get validation accuracy, loss, random configuration, and model\n",
    "            val_acc, val_loss, rand_config, model = self.training_function(X_data, y_data, X_valid, y_valid)\n",
    "\n",
    "            # Update the best parameters if the current trial performs better\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_params = rand_config\n",
    "                best_model = model\n",
    "\n",
    "        return best_acc, best_loss, best_params, best_model\n",
    "\n",
    "    def random_select(self):\n",
    "        # Randomly select hyperparameters from the configuration dictionary\n",
    "        rand_config = {}\n",
    "\n",
    "        for k, v in self.config.items():\n",
    "            m = len(v)\n",
    "            r_val = v[np.random.randint(0, m)]\n",
    "            rand_config[k] = r_val\n",
    "\n",
    "        return rand_config\n",
    "\n",
    "    def training_function(self, X_data, y_data, X_valid, y_valid):\n",
    "        # Randomly select hyperparameters\n",
    "        rand_config = self.random_select()\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        max_depth = rand_config[\"max_depth\"]\n",
    "        min_samples_per_leaf = rand_config[\"min_samples_per_leaf\"]\n",
    "        tolerance = rand_config[\"tolerance\"]\n",
    "        criterion = rand_config[\"criterion\"]\n",
    "        # Assuming target_encoder is defined elsewhere\n",
    "        classes_name = target_encoder.classes_\n",
    "\n",
    "        # Print the selected hyperparameters\n",
    "        print(f\"Currently Selected Params are criterion: {criterion}, max_depth: {max_depth}, min_samples_per_leaf: {min_samples_per_leaf}, gain_tolerance: {tolerance}\")\n",
    "\n",
    "        # Initialize and train the DecisionTreeClf model\n",
    "        model = DecisionTreeClf(criterion, max_depth, min_samples_per_leaf, classes_name, tolerance)\n",
    "        model.fit(X_data, y_data)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        preds = model.predict(X_valid)\n",
    "        # Calculate accuracy and loss\n",
    "        acc_score = accuracy_score(y_valid, preds)\n",
    "        loss = model.calculate_loss(X_valid, y_valid)\n",
    "\n",
    "        # Print validation accuracy and loss\n",
    "        print(f'The Validation Accuracy is {acc_score} and Validation loss is {loss}\\n')\n",
    "\n",
    "        return acc_score, loss, rand_config, model\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"min_samples_per_leaf\": [1, 2, 4, 8],\n",
    "    \"tolerance\": [0.1, 0.01, 0.001],\n",
    "    \"criterion\": [\"MCR\", \"Entropy\"]\n",
    "}\n",
    "\n",
    "NUM_TRIALS = 10 #number of times the random search has to be performed\n",
    "random_search = TreeRandomSearchCV(config)\n",
    "best_acc, best_loss, best_params, best_model = random_search.run(NUM_TRIALS, X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# To print the tree using breath first tree traversal till depth of 4\n",
    "best_model.print_tree(max_depth=4)\n",
    "\n",
    "#to plot information gain at each decision step\n",
    "best_model.plot_information_gain()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## USING GINI INDEX\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DecisionTreeGini:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def gini(self, y):\n",
    "        \"\"\"\n",
    "        Compute the Gini Impurity for a given set of labels y.\n",
    "        Gini = 1 - sum(p_i^2)\n",
    "        \"\"\"\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split on using Gini Impurity.\n",
    "        \"\"\"\n",
    "        best_gini = float(\"inf\")\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                left_gini = self.gini(y[left_mask])\n",
    "                right_gini = self.gini(y[right_mask])\n",
    "                weighted_gini = (np.sum(left_mask) * left_gini + np.sum(right_mask) * right_gini) / len(y)\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree using the best Gini split.\n",
    "        \"\"\"\n",
    "        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        best_feature, best_threshold = self.best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_subtree = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left_subtree, \"right\": right_subtree}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the decision tree.\n",
    "        \"\"\"\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_sample(self, sample, tree):\n",
    "        \"\"\"\n",
    "        Predict the class label for a single sample.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, dict):\n",
    "            if sample[tree[\"feature\"]] <= tree[\"threshold\"]:\n",
    "                return self.predict_sample(sample, tree[\"left\"])\n",
    "            else:\n",
    "                return self.predict_sample(sample, tree[\"right\"])\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels for all samples in X.\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_sample(sample, self.tree) for sample in X])\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "X_example = np.array([[7.2, 0], [6.4, 0], [5.9, 1], [5.0, 1]])  # Wingspan, Species\n",
    "y_example = np.array([0, 1, 0, 1])  # Female (0), Male (1)\n",
    "\n",
    "# Train Decision Tree\n",
    "tree = DecisionTreeGini(max_depth=2)\n",
    "tree.fit(X_example, y_example)\n",
    "\n",
    "# Predict on the training set\n",
    "y_pred_example = tree.predict(X_example)\n",
    "\n",
    "# Display tree structure and predictions\n",
    "tree_structure = tree.tree\n",
    "prediction_results = pd.DataFrame({\"True Label\": y_example, \"Predicted Label\": y_pred_example})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Decision Tree Predictions\", dataframe=prediction_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea424cd5-022c-4a75-ab58-6f78270c458a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06af5ed4-056f-46a8-ab06-b412991a3875",
   "metadata": {},
   "source": [
    "wingspan species sex\n",
    "7.2ft bald eagle female\n",
    "6.4ft bald eagle male\n",
    "5.9ft osprey female\n",
    "5.0ft osprey male\n",
    "\n",
    "Construct a decision tree (without explicit training) which realizes the partition shown in Figure 1.\r\n",
    "B. [4p] Given the data from Table 1, train by hand a Decision Tree Classifier that predicts the sex of a bird\r\n",
    "given its wingspan and species. Use the Gini-Index as the splitting criterion. Draw the learned tree.\r\n",
    "C. [2p] Find and draw a minimal depth Decision Tree that solves the classification problem from 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b5f8a-84d8-4814-b855-7c6a9fa3847d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b92299-38e5-4bf4-9210-17c12d4fa138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Given dataset (Table 1)\n",
    "wingspan = np.array([7.2, 6.4, 5.9, 5.0]).reshape(-1, 1)  # Feature: Wingspan\n",
    "species = np.array([0, 0, 1, 1])  # Bald eagle (0), Osprey (1) as numerical labels\n",
    "sex = np.array([0, 1, 0, 1])  # Female (0), Male (1)\n",
    "\n",
    "# Combining features (Wingspan + Species)\n",
    "X = np.column_stack((wingspan, species))\n",
    "y = sex  # Target: Sex\n",
    "\n",
    "# Train a Decision Tree Classifier using Gini Index\n",
    "tree_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=2)\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "# Plot the Decision Tree\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_tree(tree_model, feature_names=[\"Wingspan\", \"Species\"], class_names=[\"Female\", \"Male\"], filled=True)\n",
    "plt.title(\"Minimal Depth Decision Tree\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ecaac-6eea-4976-b007-28bdbb439f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372c6b8-f386-416c-9a3e-770f258ed8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Jaccard Similarity and Hamming Distance\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"\n",
    "    Compute Jaccard Similarity between two sets.\n",
    "    Jaccard Similarity = |Intersection| / |Union|\n",
    "    \"\"\"\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def hamming_distance(str1, str2):\n",
    "    \"\"\"\n",
    "    Compute Hamming Distance between two strings of equal length.\n",
    "    Hamming Distance = Count of positions where characters differ\n",
    "    \"\"\"\n",
    "    if len(str1) != len(str2):\n",
    "        raise ValueError(\"Strings must be of the same length for Hamming distance\")\n",
    "\n",
    "    return sum(c1 != c2 for c1, c2 in zip(str1, str2))\n",
    "\n",
    "# Example Usage\n",
    "set_a = {\"apple\", \"banana\", \"cherry\"}\n",
    "set_b = {\"banana\", \"cherry\", \"date\", \"fig\"}\n",
    "jaccard_score = jaccard_similarity(set_a, set_b)\n",
    "\n",
    "str1 = \"karolin\"\n",
    "str2 = \"kathrin\"\n",
    "hamming_dist = hamming_distance(str1, str2)\n",
    "\n",
    "# Display results\n",
    "distance_results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Jaccard Similarity\", \"Hamming Distance\"],\n",
    "    \"Value\": [jaccard_score, hamming_dist]\n",
    "})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Similarity and Distance Metrics\", dataframe=distance_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65fcff-630d-4e49-8fbe-d6504b1e9baf",
   "metadata": {},
   "source": [
    "###### Your task is to implement a function called levenshtein_dist, that takes two input strings and calculates the distance between them. Ultimately, you need to find the Levenshtein distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5ee90-fc4c-4461-a98d-57bc9f503bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_dist(s1, s2):\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein distance between two strings using dynamic programming.\n",
    "    The Levenshtein distance is the minimum number of single-character edits (insertions, deletions, or substitutions)\n",
    "    required to change one string into the other.\n",
    "    \"\"\"\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    \n",
    "    # Initialize distance matrix\n",
    "    dp = np.zeros((len_s1 + 1, len_s2 + 1), dtype=int)\n",
    "\n",
    "    # Fill the base case values\n",
    "    for i in range(len_s1 + 1):\n",
    "        dp[i][0] = i  # Cost of deleting all characters\n",
    "    for j in range(len_s2 + 1):\n",
    "        dp[0][j] = j  # Cost of inserting all characters\n",
    "\n",
    "    # Compute Levenshtein distance\n",
    "    for i in range(1, len_s1 + 1):\n",
    "        for j in range(1, len_s2 + 1):\n",
    "            if s1[i - 1] == s2[j - 1]:  # Characters are the same\n",
    "                cost = 0\n",
    "            else:  # Characters are different (substitution cost = 1)\n",
    "                cost = 1\n",
    "\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,      # Deletion\n",
    "                           dp[i][j - 1] + 1,      # Insertion\n",
    "                           dp[i - 1][j - 1] + cost)  # Substitution\n",
    "\n",
    "    return dp[len_s1][len_s2]\n",
    "\n",
    "# Example usage\n",
    "string1 = \"kitten\"\n",
    "string2 = \"sitting\"\n",
    "\n",
    "levenshtein_result = levenshtein_dist(string1, string2)\n",
    "\n",
    "# Display result\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Levenshtein Distance\"],\n",
    "    \"Value\": [levenshtein_result]\n",
    "})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Levenshtein Distance Result\", dataframe=results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d31de-4c76-4a3d-b8ea-88295caf4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### We will now implement the Manhattan, Euclidian and Chebyshev distance metrics \n",
    "\n",
    "# Implement Manhattan Distance\n",
    "def manhattan_distance(p1, p2):\n",
    "    \"\"\"\n",
    "    Compute the Manhattan distance between two points.\n",
    "    Manhattan Distance = sum(|x_i - y_i|)\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(np.array(p1) - np.array(p2)))\n",
    "\n",
    "# Implement Euclidean Distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two points.\n",
    "    Euclidean Distance = sqrt(sum((x_i - y_i)^2))\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((np.array(p1) - np.array(p2)) ** 2))\n",
    "\n",
    "# Implement Chebyshev Distance\n",
    "def chebyshev_distance(p1, p2):\n",
    "    \"\"\"\n",
    "    Compute the Chebyshev distance between two points.\n",
    "    Chebyshev Distance = max(|x_i - y_i|)\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(np.array(p1) - np.array(p2)))\n",
    "\n",
    "# Example usage\n",
    "point1 = (3, 5, 7)\n",
    "point2 = (1, 9, 2)\n",
    "\n",
    "# Compute distances\n",
    "manhattan_result = manhattan_distance(point1, point2)\n",
    "euclidean_result = euclidean_distance(point1, point2)\n",
    "chebyshev_result = chebyshev_distance(point1, point2)\n",
    "\n",
    "# Display results\n",
    "distance_results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Manhattan Distance\", \"Euclidean Distance\", \"Chebyshev Distance\"],\n",
    "    \"Value\": [manhattan_result, euclidean_result, chebyshev_result]\n",
    "})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Distance Metrics Results\", dataframe=distance_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7042d7-a2cb-40e5-8046-f56b9bea2fea",
   "metadata": {},
   "source": [
    "In this part, we will use Polynomial regression to understand how over and under-fitting works. Using the below given code, you need to generate multiple datasets. The sizes of the datasets should be \n",
    ".\n",
    "\n",
    "Create a Polynomial Feature Transformer. This is a preprocessing step that simply takes in a value (x) and returns its higher powers upto the degree D (i.e. \n",
    ").\n",
    "Transform the x features using the Polynomial Feature Transformer.\n",
    "Now, if we fit a linear regressor using these transformed features, we are in essence creating a polynomial regressor. For this, create an object class Linear_Regressor that can be fitted on arbitary number of features. You can use any algorithm to create this class (sklearn is not allowed). Use MSE loss for this. (Remember not to forget the bias term)\n",
    "Perform Hyper-parameter tunning using this Linear_Regressor. You are to try out different degree models for all datasets. (Use at least 10 different degree models ranging between 1 and 20). Compare the results on validation data to find the best degree models for each size of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e42183-c977-44af-a002-dd75efb63e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate Synthetic Dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100)\n",
    "y = X**3 - 3*X**2 + 2*X + np.random.normal(0, 3, size=100)  # True polynomial function with noise\n",
    "\n",
    "X = X.reshape(-1, 1)  # Reshape for compatibility\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Implement Polynomial Feature Transformer\n",
    "class PolynomialFeatureTransformer:\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms input X into polynomial features up to the given degree.\n",
    "        \"\"\"\n",
    "        poly_features = np.ones((X.shape[0], 1))  # Bias term (x^0)\n",
    "        for d in range(1, self.degree + 1):\n",
    "            poly_features = np.hstack((poly_features, X ** d))\n",
    "        return poly_features\n",
    "\n",
    "# Implement Linear Regressor using Normal Equation\n",
    "class LinearRegressor:\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train linear regression using the normal equation.\n",
    "        \"\"\"\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n",
    "        self.coefficients = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the trained model.\n",
    "        \"\"\"\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n",
    "        return X_b.dot(self.coefficients)\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error (MSE).\n",
    "        \"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Hyperparameter tuning: Testing different polynomial degrees\n",
    "degrees = list(range(1, 21))  # Degrees from 1 to 20\n",
    "validation_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Transform features\n",
    "    transformer = PolynomialFeatureTransformer(degree)\n",
    "    X_train_poly = transformer.transform(X_train)\n",
    "    X_val_poly = transformer.transform(X_val)\n",
    "\n",
    "    # Train model\n",
    "    model = LinearRegressor()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val_poly)\n",
    "\n",
    "    # Compute validation error\n",
    "    mse_val = model.mse(y_val, y_val_pred)\n",
    "    validation_errors.append(mse_val)\n",
    "\n",
    "# Find best degree (with lowest validation error)\n",
    "best_degree = degrees[np.argmin(validation_errors)]\n",
    "\n",
    "# Train final model with best degree on full dataset\n",
    "best_transformer = PolynomialFeatureTransformer(best_degree)\n",
    "X_train_poly_best = best_transformer.transform(X_train)\n",
    "X_test_poly_best = best_transformer.transform(X_test)\n",
    "\n",
    "final_model = LinearRegressor()\n",
    "final_model.fit(X_train_poly_best, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = final_model.predict(X_test_poly_best)\n",
    "\n",
    "# Compute final test error\n",
    "final_test_mse = final_model.mse(y_test, y_test_pred)\n",
    "\n",
    "# Plot validation error vs degree\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(degrees, validation_errors, marker='o', linestyle='dashed', color='b', label=\"Validation Error\")\n",
    "plt.axvline(best_degree, color='r', linestyle='--', label=f\"Best Degree = {best_degree}\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Validation MSE\")\n",
    "plt.title(\"Hyperparameter Tuning: Degree Selection\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "final_results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Best Polynomial Degree\", \"Final Test MSE\"],\n",
    "    \"Value\": [best_degree, final_test_mse]\n",
    "})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Polynomial Regression Results\", dataframe=final_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1c8e2-65a2-45c3-91c9-4377ae0dc19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "735d3519-a335-4adb-a993-3a7ae0595d76",
   "metadata": {},
   "source": [
    "Plot the train/val/test MSE losses for each degree model per data-size. (This would be one plot per data size).\n",
    "Visualize the test results of all these different models vs data sizes. (This should be a single plot).\n",
    "Comment on how the degree hyperparameter changes as data size is increased. Also create a visualization to show the best degree model for each datasize. (This should be a single plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73065b38-e54a-42bc-bd66-38b464e51adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different dataset sizes for experimentation\n",
    "dataset_sizes = [20, 50, 100]  # Different numbers of training points\n",
    "degrees = list(range(1, 21))  # Degrees from 1 to 20\n",
    "\n",
    "# Store results\n",
    "mse_results = {size: {\"train\": [], \"val\": [], \"test\": []} for size in dataset_sizes}\n",
    "best_degrees = []\n",
    "\n",
    "# Loop over different dataset sizes\n",
    "for size in dataset_sizes:\n",
    "    # Subset the training data\n",
    "    X_train_subset = X_train[:size]\n",
    "    y_train_subset = y_train[:size]\n",
    "\n",
    "    val_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    # Test different polynomial degrees\n",
    "    for degree in degrees:\n",
    "        transformer = PolynomialFeatureTransformer(degree)\n",
    "        X_train_poly = transformer.transform(X_train_subset)\n",
    "        X_val_poly = transformer.transform(X_val)\n",
    "        X_test_poly = transformer.transform(X_test)\n",
    "\n",
    "        model = LinearRegressor()\n",
    "        model.fit(X_train_poly, y_train_subset)\n",
    "\n",
    "        y_train_pred = model.predict(X_train_poly)\n",
    "        y_val_pred = model.predict(X_val_poly)\n",
    "        y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "        # Compute MSE for train, val, and test sets\n",
    "        mse_train = model.mse(y_train_subset, y_train_pred)\n",
    "        mse_val = model.mse(y_val, y_val_pred)\n",
    "        mse_test = model.mse(y_test, y_test_pred)\n",
    "\n",
    "        mse_results[size][\"train\"].append(mse_train)\n",
    "        mse_results[size][\"val\"].append(mse_val)\n",
    "        mse_results[size][\"test\"].append(mse_test)\n",
    "        \n",
    "        val_errors.append(mse_val)\n",
    "        test_errors.append(mse_test)\n",
    "\n",
    "    # Find the best polynomial degree for this dataset size\n",
    "    best_degree = degrees[np.argmin(val_errors)]\n",
    "    best_degrees.append(best_degree)\n",
    "\n",
    "# Plot MSE losses for each dataset size\n",
    "for size in dataset_sizes:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(degrees, mse_results[size][\"train\"], label=\"Train MSE\", marker='o', linestyle='dotted')\n",
    "    plt.plot(degrees, mse_results[size][\"val\"], label=\"Validation MSE\", marker='o', linestyle='dashed')\n",
    "    plt.plot(degrees, mse_results[size][\"test\"], label=\"Test MSE\", marker='o', linestyle='solid')\n",
    "    \n",
    "    plt.axvline(best_degrees[dataset_sizes.index(size)], color='r', linestyle='--', label=f\"Best Degree={best_degrees[dataset_sizes.index(size)]}\")\n",
    "    plt.xlabel(\"Polynomial Degree\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(f\"MSE Loss vs Polynomial Degree (Dataset Size = {size})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize test results for all models vs dataset size\n",
    "plt.figure(figsize=(8, 6))\n",
    "for size in dataset_sizes:\n",
    "    plt.plot(degrees, mse_results[size][\"test\"], marker='o', linestyle='solid', label=f\"Test MSE (Size={size})\")\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.title(\"Test MSE vs Polynomial Degree for Different Dataset Sizes\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualizing best degree models for each dataset size\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(dataset_sizes, best_degrees, marker='o', linestyle='dashed', color='b', label=\"Best Degree\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Best Polynomial Degree\")\n",
    "plt.title(\"Optimal Polynomial Degree vs Dataset Size\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Comment on how the degree changes with data size\n",
    "commentary = \"\"\"\n",
    "As dataset size increases, the best polynomial degree also tends to increase. \n",
    "For smaller datasets, a lower-degree polynomial generalizes better, avoiding overfitting.\n",
    "Larger datasets allow more complex models (higher-degree polynomials) without overfitting.\n",
    "This demonstrates how increasing data size allows for capturing more complex patterns while avoiding high variance.\n",
    "\"\"\"\n",
    "\n",
    "# Display the commentary\n",
    "commentary_df = pd.DataFrame({\"Observation\": [commentary]})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Observations on Degree vs Dataset Size\", dataframe=commentary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4194e2-42f9-4bee-bbcd-0167b740d5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0cfca-6660-43fd-b35b-64ed72a2c526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f68f7eab-117d-4830-be9d-cfa89210b640",
   "metadata": {},
   "source": [
    "Variable selection via forward and backward search drops some predictors; in some cases, we don't want to remove these predictors. Rather we want their coefficients to be small as possible. We are going to test the effect of the regularization term alpha. We will use the same data as in Variable Selection. \n",
    "\n",
    "Furthermore, we will use following sklearn implementation: \n",
    "- Ridge regression (sklearn.linear_model.Ridge)\n",
    "- Lasso (sklearn.linear_model.Lasso)\n",
    "- Elastic-Net (sklearn.linear_model.ElasticNet)\n",
    "\n",
    "You need to implement GridSearch and RandomSearch algorithms to tune the value of $\\alpha$. In both case try 5 different values of $\\alpha$. \n",
    "\n",
    "Remember to use the validation set to find the best hyperparameters.\n",
    "\n",
    "- Plot a graph of $\\alpha$ values vs loss for all models (one for GridSearch and one for RandomSearch). Also report the test loss for the all three models using the best $\\alpha$ values.\n",
    "  \n",
    "- Select the least test loss model from both of the above parts:\n",
    "- Model A: from Variable Selection\n",
    "- Model B: from Regularization and Hyperparameter Search\n",
    "\n",
    "Find the list of variables not selected in Model A, use this list to separate the coefficients of the model B into two groups. Comment on the difference in the values of the coefficients of these groups.\n",
    "\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c304439-5e52-42d8-b631-171b2f54fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########yperparameter search\n",
    "\n",
    "ridge = Ridge\n",
    "elastic_net = ElasticNet\n",
    "lasso = Lasso\n",
    "clf_list = [ridge,elastic_net,lasso]\n",
    "\n",
    "#param grid with different values of alpha\n",
    "params = {'alpha':[10,1,0.1,0.0001,0.00001]}\n",
    "\n",
    "class HyperparameterSearch:\n",
    "    def __init__(self, models, alpha_values):\n",
    "        # Initialize the HyperparameterSearch class with a list of models and alpha values\n",
    "        self.models = models\n",
    "        self.alphas = alpha_values\n",
    "\n",
    "    def grid_search(self, X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "        # Perform grid search for hyperparameter tuning\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        best_mse = float('inf')\n",
    "        model_list = {}\n",
    "\n",
    "        # Iterate through each model\n",
    "        for model in self.models:\n",
    "            model_list[model.__name__] = [[], []]\n",
    "            model_loss = float('inf')\n",
    "\n",
    "            # Iterate through each alpha value\n",
    "            for alpha in self.alphas:\n",
    "                current_params = alpha\n",
    "\n",
    "                # Create the model with the current alpha\n",
    "                current_model = model(alpha=current_params)\n",
    "                current_model.fit(X_train, y_train)\n",
    "                \n",
    "                # Make predictions on the validation set\n",
    "                y_pred = current_model.predict(X_valid)\n",
    "                \n",
    "                # Calculate mean squared error\n",
    "                current_mse = mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "                # Store alpha and corresponding MSE in the model_list\n",
    "                model_list[model.__name__][0].append(current_params)\n",
    "                model_list[model.__name__][1].append(current_mse)\n",
    "\n",
    "                # Update the best model if the current MSE is better\n",
    "                if current_mse < best_mse:\n",
    "                    best_mse = current_mse\n",
    "                    best_params = current_params\n",
    "                    best_model = current_model\n",
    "                    \n",
    "                if current_mse < model_loss:\n",
    "                    best_alpha = alpha\n",
    "                    best_alpha_model = current_model\n",
    "            \n",
    "            print(f\"Best test loss for {model.__name__} for best alpha {best_alpha} is:\\\n",
    "            {mean_squared_error(y_test,current_model.predict(X_test))}\")\n",
    "                \n",
    "\n",
    "        # Save the history of grid search\n",
    "        self.grid_history = model_list\n",
    "        return best_params, best_model\n",
    "\n",
    "    def random_search(self, X_train, y_train, X_valid, y_valid, X_test, y_test, num_trials=3):\n",
    "        # Perform random search for hyperparameter tuning\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        best_mse = float('inf')\n",
    "        model_list = {}\n",
    "\n",
    "        # Iterate through each model\n",
    "        for model in self.models:\n",
    "            model_list[model.__name__] = [[], []]\n",
    "\n",
    "            # Iterate for a specified number of random trials\n",
    "            for _ in range(num_trials):\n",
    "                alpha = random.choice(self.alphas)\n",
    "\n",
    "                current_params = alpha\n",
    "                current_model = model(alpha=current_params)\n",
    "                current_model.fit(X_train, y_train)\n",
    "                y_pred = current_model.predict(X_valid)\n",
    "                current_mse = mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "                (model_list[model.__name__][0]).append(current_params)\n",
    "                (model_list[model.__name__][1]).append(current_mse)\n",
    "\n",
    "                # Update the best model if the current MSE is better\n",
    "                if current_mse < best_mse:\n",
    "                    best_mse = current_mse\n",
    "                    best_params = current_params\n",
    "                    best_model = current_model\n",
    "\n",
    "        # Save the history of random search\n",
    "        self.rand_history = model_list\n",
    "        return best_params, best_model\n",
    "    \n",
    "    def plot_loss_vs_alpha(self, search='grid search'):\n",
    "        # Plot the loss vs alpha for different models\n",
    "        if search == 'grid search':\n",
    "            models_data = self.grid_history\n",
    "        else:\n",
    "            models_data = self.rand_history\n",
    "\n",
    "        for model_name, data in models_data.items():\n",
    "            alphas = data[0]\n",
    "            loss_values = data[1]\n",
    "            plt.plot(alphas, loss_values,'o', label=model_name)\n",
    "\n",
    "        plt.xlabel('Alpha Values')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss vs Alpha for Different Models using {search}')\n",
    "        plt.legend()\n",
    "        plt.xscale('log')  # Set x-axis to a logarithmic scale for better visualization\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the HyperparameterSearch class\n",
    "hyperparameter_search = HyperparameterSearch(clf_list, params['alpha'])\n",
    "\n",
    "# Perform grid search\n",
    "grid_best_params, grid_best_model = hyperparameter_search.grid_search(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "# Perform random search\n",
    "random_best_params, random_best_model = hyperparameter_search.random_search(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Grid Search - Best Model:\", grid_best_model.__class__.__module__.split('.')[-1])\n",
    "print(\"Grid Search - Best Hyperparameters:\", grid_best_params)\n",
    "print(\"Grid Search - Validation MSE:\", mean_squared_error(y_valid, grid_best_model.predict(X_valid)))\n",
    "print(\"\\nRandom Search - Best Model:\", random_best_model.__class__.__module__.split('.')[-1])\n",
    "print(\"Random Search - Best Hyperparameters:\", random_best_params)\n",
    "print(\"Random Search - Validation MSE:\", mean_squared_error(y_valid, random_best_model.predict(X_valid)))\n",
    "\n",
    "\n",
    "### write your code here\n",
    "hyperparameter_search.plot_loss_vs_alpha()\n",
    "hyperparameter_search.plot_loss_vs_alpha(search='random search')\n",
    "\n",
    "\n",
    "\n",
    "### write your code here\n",
    "print(\"Selected Features by Model A Coefficents\",grid_best_model.coef_[forward_selected_features])\n",
    "not_useful_variables = list(set(range(X_train.shape[1])).difference(set(forward_selected_features)))\n",
    "print(\"\\nNot selected features by Model A Coefficents\",grid_best_model.coef_[not_useful_variables])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcedfc1-02b3-4ed8-877b-b1a83edb7a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
